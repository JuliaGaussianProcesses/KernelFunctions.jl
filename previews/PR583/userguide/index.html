<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>User guide · KernelFunctions.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">KernelFunctions.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>User guide</a><ul class="internal"><li><a class="tocitem" href="#Kernel-Creation"><span>Kernel Creation</span></a></li><li><a class="tocitem" href="#Using-a-Kernel-Function"><span>Using a Kernel Function</span></a></li><li><a class="tocitem" href="#Creating-a-Kernel-Matrix"><span>Creating a Kernel Matrix</span></a></li><li><a class="tocitem" href="#Composite-Kernels"><span>Composite Kernels</span></a></li><li><a class="tocitem" href="#Kernel-Parameters"><span>Kernel Parameters</span></a></li></ul></li><li><a class="tocitem" href="../kernels/">Kernel Functions</a></li><li><a class="tocitem" href="../transform/">Input Transforms</a></li><li><a class="tocitem" href="../metrics/">Metrics</a></li><li><a class="tocitem" href="../create_kernel/">Custom Kernels</a></li><li><a class="tocitem" href="../api/">API</a></li><li><a class="tocitem" href="../design/">Design</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/gaussian-process-priors/">Gaussian process prior samples</a></li><li><a class="tocitem" href="../examples/kernel-ridge-regression/">Kernel Ridge Regression</a></li><li><a class="tocitem" href="../examples/support-vector-machine/">Support Vector Machine</a></li><li><a class="tocitem" href="../examples/train-kernel-parameters/">Train Kernel Parameters</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>User guide</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>User guide</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaGaussianProcesses/KernelFunctions.jl/blob/master/docs/src/userguide.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="User-guide"><a class="docs-heading-anchor" href="#User-guide">User guide</a><a id="User-guide-1"></a><a class="docs-heading-anchor-permalink" href="#User-guide" title="Permalink"></a></h1><h2 id="Kernel-Creation"><a class="docs-heading-anchor" href="#Kernel-Creation">Kernel Creation</a><a id="Kernel-Creation-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-Creation" title="Permalink"></a></h2><p>To create a kernel object, choose one of the pre-implemented kernels, see <a href="../kernels/#Kernel-Functions">Kernel Functions</a>, or create your own, see <a href="../create_kernel/#Creating-your-own-kernel">Creating your own kernel</a>. For example, a squared exponential kernel is created by</p><pre><code class="language-julia hljs">  k = SqExponentialKernel()</code></pre><div class="admonition is-success"><header class="admonition-header">How do I set the lengthscale(s)?</header><div class="admonition-body"><p>Instead of having lengthscale(s) for each kernel we use <a href="../transform/#KernelFunctions.Transform"><code>Transform</code></a> objects which act on the inputs before passing them to the kernel. Note that the transforms such as <a href="../transform/#KernelFunctions.ScaleTransform"><code>ScaleTransform</code></a> and <a href="../transform/#KernelFunctions.ARDTransform"><code>ARDTransform</code></a> <em>multiply</em> the input by a scale factor, which corresponds to the <em>inverse</em> of the lengthscale. For example, a lengthscale of 0.5 is equivalent to premultiplying the input by 2.0, and you can create the corresponding kernel in either of the following equivalent ways:</p><pre><code class="language-julia hljs">  k = SqExponentialKernel() ∘ ScaleTransform(2.0)
  k = compose(SqExponentialKernel(), ScaleTransform(2.0))</code></pre><p>Alternatively, you can use the convenience function <a href="../transform/#KernelFunctions.with_lengthscale"><code>with_lengthscale</code></a>:</p><pre><code class="language-julia hljs">k = with_lengthscale(SqExponentialKernel(), 0.5)</code></pre><p><a href="../transform/#KernelFunctions.with_lengthscale"><code>with_lengthscale</code></a> also works with vector-valued lengthscales for multiple-dimensional inputs, and is equivalent to pre-composing with an <a href="../transform/#KernelFunctions.ARDTransform"><code>ARDTransform</code></a>:</p><pre><code class="language-julia hljs">length_scales = [1.0, 2.0]
k = with_lengthscale(SqExponentialKernel(), length_scales)
k = SqExponentialKernel() ∘ ARDTransform(1 ./ length_scales)</code></pre><p>Check the <a href="../transform/#input_transforms">Input Transforms</a> page for more details.</p></div></div><div class="admonition is-success"><header class="admonition-header">How do I set the kernel variance?</header><div class="admonition-body"><p>To premultiply the kernel by a variance, you can use <code>*</code> with a scalar number:</p><pre><code class="language-julia hljs">  k = 3.0 * SqExponentialKernel()</code></pre></div></div><div class="admonition is-success"><header class="admonition-header">How do I use a Mahalanobis kernel?</header><div class="admonition-body"><p>The <code>MahalanobisKernel(; P=P)</code>, defined by</p><p class="math-container">\[k(x, x&#39;; P) = \exp{\big(- (x - x&#39;)^\top P (x - x&#39;)\big)}\]</p><p>for a positive definite matrix <span>$P = Q^\top Q$</span>, was removed in 0.9. Instead you can use a squared exponential kernel together with a <a href="../transform/#KernelFunctions.LinearTransform"><code>LinearTransform</code></a> of the inputs:</p><pre><code class="language-julia hljs">k = SqExponentialKernel() ∘ LinearTransform(sqrt(2) .* Q)</code></pre><p>Analogously, you can combine other kernels such as the <a href="../kernels/#KernelFunctions.PiecewisePolynomialKernel"><code>PiecewisePolynomialKernel</code></a> with a <a href="../transform/#KernelFunctions.LinearTransform"><code>LinearTransform</code></a> of the inputs to obtain a kernel that is a function of the Mahalanobis distance between inputs.</p></div></div><h2 id="Using-a-Kernel-Function"><a class="docs-heading-anchor" href="#Using-a-Kernel-Function">Using a Kernel Function</a><a id="Using-a-Kernel-Function-1"></a><a class="docs-heading-anchor-permalink" href="#Using-a-Kernel-Function" title="Permalink"></a></h2><p>To evaluate the kernel function on two vectors you simply call the kernel object:</p><pre><code class="language-julia hljs">k = SqExponentialKernel()
x1 = rand(3)
x2 = rand(3)
k(x1, x2)</code></pre><h2 id="Creating-a-Kernel-Matrix"><a class="docs-heading-anchor" href="#Creating-a-Kernel-Matrix">Creating a Kernel Matrix</a><a id="Creating-a-Kernel-Matrix-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-a-Kernel-Matrix" title="Permalink"></a></h2><p>Kernel matrices can be created via the <code>kernelmatrix</code> function or <code>kernelmatrix_diag</code> for only the diagonal. For example, for a collection of 10 <code>Real</code>-valued inputs:</p><pre><code class="language-julia hljs">k = SqExponentialKernel()
x = rand(10)
kernelmatrix(k, x) # 10x10 matrix</code></pre><p>If your inputs are multi-dimensional, it is common to represent them as a matrix. For example</p><pre><code class="language-julia hljs">X = rand(10, 5)</code></pre><p>However, it is ambiguous whether this represents a collection of 10 5-dimensional row-vectors, or 5 10-dimensional column-vectors. Therefore, we require users to provide some more information.</p><p>You can write <code>RowVecs(X)</code> to declare that <code>X</code> contains 10 5-dimensional row-vectors, or <code>ColVecs(X)</code> to declare that <code>X</code> contains 5 10-dimensional column-vectors, then</p><pre><code class="language-julia hljs">kernelmatrix(k, RowVecs(X))  # returns a 10×10 matrix -- each row of X treated as input
kernelmatrix(k, ColVecs(X))  # returns a 5×5 matrix -- each column of X treated as input</code></pre><p>This is the mechanism used throughout KernelFunctions.jl to handle multi-dimensional inputs.</p><p>You can utilise the <code>obsdim</code> keyword argument if you prefer:</p><pre><code class="language-julia hljs">kernelmatrix(k, X; obsdim=1) # same as RowVecs(X)
kernelmatrix(k, X; obsdim=2) # same as ColVecs(X)</code></pre><p>This is similar to the convention used in <a href="https://github.com/JuliaStats/Distances.jl">Distances.jl</a>.</p><h3 id="So-what-type-should-I-use-to-represent-a-collection-of-inputs?"><a class="docs-heading-anchor" href="#So-what-type-should-I-use-to-represent-a-collection-of-inputs?">So what type should I use to represent a collection of inputs?</a><a id="So-what-type-should-I-use-to-represent-a-collection-of-inputs?-1"></a><a class="docs-heading-anchor-permalink" href="#So-what-type-should-I-use-to-represent-a-collection-of-inputs?" title="Permalink"></a></h3><p>The central assumption made by KernelFunctions.jl is that all collections of <code>N</code> inputs are represented by <code>AbstractVector</code>s of length <code>N</code>. Abstraction is then used to ensure that efficiency is retained, <code>ColVecs</code> and <code>RowVecs</code> being the most obvious examples of this.</p><p>Concretely:</p><ol><li>For <code>Real</code>-valued inputs (scalars), a <code>Vector{&lt;:Real}</code> is fine.</li><li>For vector-valued inputs, consider a <code>ColVecs</code> or <code>RowVecs</code>.</li><li>For a new input type, simply represent collections of inputs of this type as an <code>AbstractVector</code>.</li></ol><p>See <a href="../api/#Input-Types">Input Types</a> and <a href="../design/#Design">Design</a> for a more thorough discussion of the considerations made when this design was adopted.</p><p>The <code>obsdim</code> kwarg mentioned above is a special case for vector-valued inputs stored in a matrix. It is implemented as a lightweight wrapper that constructs either a <code>RowVecs</code> or <code>ColVecs</code> from your inputs, and passes this on.</p><h3 id="Output-Types"><a class="docs-heading-anchor" href="#Output-Types">Output Types</a><a id="Output-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Output-Types" title="Permalink"></a></h3><p>In addition to plain <code>Matrix</code>-like output, KernelFunctions.jl supports specific output types:</p><ul><li>For a positive-definite matrix object of type <code>PDMat</code> from <a href="https://github.com/JuliaStats/PDMats.jl"><code>PDMats.jl</code></a>, you can call the following:</li></ul><pre><code class="language-julia hljs">using PDMats
k = SqExponentialKernel()
K = kernelpdmat(k, RowVecs(X)) # PDMat
K = kernelpdmat(k, X; obsdim=1) # PDMat</code></pre><p>It will create a matrix and in case of bad conditioning will add some diagonal noise until the matrix is considered positive-definite; it will then return a <code>PDMat</code> object. For this method to work in your code you need to include <code>using PDMats</code> first.</p><ul><li>For a Kronecker matrix, we rely on <a href="https://github.com/MichielStock/Kronecker.jl"><code>Kronecker.jl</code></a>. Here are two examples:</li></ul><pre><code class="language-julia hljs">using Kronecker
x = range(0, 1; length=10)
y = range(0, 1; length=50)
K = kernelkronmat(k, [x, y]) # Kronecker matrix
K = kernelkronmat(k, x, 5) # Kronecker matrix</code></pre><p>Make sure that <code>k</code> is a kernel compatible with such constructions (with <code>iskroncompatible(k)</code>). Both methods will return a Kronecker matrix. For those methods to work in your code you need to include <code>using Kronecker</code> first.</p><ul><li>For a Nystrom approximation: <code>kernelmatrix(nystrom(k, X, ρ, obsdim=1))</code> where <code>ρ</code> is the fraction of data samples used in the approximation.</li></ul><h2 id="Composite-Kernels"><a class="docs-heading-anchor" href="#Composite-Kernels">Composite Kernels</a><a id="Composite-Kernels-1"></a><a class="docs-heading-anchor-permalink" href="#Composite-Kernels" title="Permalink"></a></h2><p>Sums and products of kernels are also valid kernels. They can be created via <code>KernelSum</code> and <code>KernelProduct</code> or using simple operators <code>+</code> and <code>*</code>. For example:</p><pre><code class="language-julia hljs">k1 = SqExponentialKernel()
k2 = Matern32Kernel()
k = 0.5 * k1 + 0.2 * k2 # KernelSum
k = k1 * k2 # KernelProduct</code></pre><h2 id="Kernel-Parameters"><a class="docs-heading-anchor" href="#Kernel-Parameters">Kernel Parameters</a><a id="Kernel-Parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-Parameters" title="Permalink"></a></h2><p>What if you want to differentiate through the kernel parameters? This is easy even in a highly nested structure such as:</p><pre><code class="language-julia hljs">k = (
    0.5 * SqExponentialKernel() * Matern12Kernel() +
    0.2 * (LinearKernel() ∘ ScaleTransform(2.0) + PolynomialKernel())
) ∘ ARDTransform([0.1, 0.5])</code></pre><p>One can access the named tuple of trainable parameters via <code>Functors.functor</code> from <code>Functors.jl</code>. This means that in practice you can implicitly optimize the kernel parameters by calling:</p><pre><code class="language-julia hljs">using Flux
kernelparams = Flux.params(k)
Flux.gradient(kernelparams) do
    # ... some loss function on the kernel ....
end</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../kernels/">Kernel Functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Tuesday 13 January 2026 11:47">Tuesday 13 January 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
