var documenterSearchIndex = {"docs":
[{"location":"metrics/#Metrics","page":"Metrics","title":"Metrics","text":"","category":"section"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"KernelFunctions.jl relies on Distances.jl for computing the pairwise matrix. To do so a distance measure is needed for each kernel. Two very common ones can already be used : SqEuclidean and Euclidean. However all kernels do not rely on distances metrics respecting all the definitions. That's why  additional metrics come with the package such as DotProduct (<x,y>) and Delta (δ(x,y)). Note that every SimpleKernel must have a defined metric defined as :","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"    KernelFunctions.metric(::CustomKernel) = SqEuclidean()","category":"page"},{"location":"metrics/#Adding-a-new-metric","page":"Metrics","title":"Adding a new metric","text":"","category":"section"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"If you want to create a new distance just implement the following :","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"struct Delta <: Distances.PreMetric\nend\n\n@inline function Distances._evaluate(::Delta,a::AbstractVector{T},b::AbstractVector{T}) where {T}\n    @boundscheck if length(a) != length(b)\n        throw(DimensionMismatch(\"first array has length $(length(a)) which does not match the length of the second, $(length(b)).\"))\n    end\n    return a==b\nend\n\n@inline (dist::Delta)(a::AbstractArray,b::AbstractArray) = Distances._evaluate(dist,a,b)\n@inline (dist::Delta)(a::Number,b::Number) = a==b","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  CurrentModule = KernelFunctions","category":"page"},{"location":"kernels/#Base-Kernels","page":"Kernel Functions","title":"Base Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"These are the basic kernels without any transformation of the data. They are the building blocks of KernelFunctions","category":"page"},{"location":"kernels/#Constant-Kernels","page":"Kernel Functions","title":"Constant Kernels","text":"","category":"section"},{"location":"kernels/#Constant-Kernel","page":"Kernel Functions","title":"Constant Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The ConstantKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxc) = c","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where c in mathbbR.","category":"page"},{"location":"kernels/#White-Kernel","page":"Kernel Functions","title":"White Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The WhiteKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = delta(x-x)","category":"page"},{"location":"kernels/#Zero-Kernel","page":"Kernel Functions","title":"Zero Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The ZeroKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = 0","category":"page"},{"location":"kernels/#Cosine-Kernel","page":"Kernel Functions","title":"Cosine Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The CosineKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(x x) = cos(pi x-x)","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where xinmathbbR.","category":"page"},{"location":"kernels/#Exponential-Kernels","page":"Kernel Functions","title":"Exponential Kernels","text":"","category":"section"},{"location":"kernels/#Exponential-Kernel","page":"Kernel Functions","title":"Exponential Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The ExponentialKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = expleft(-x-xright)","category":"page"},{"location":"kernels/#Square-Exponential-Kernel","page":"Kernel Functions","title":"Square Exponential Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The SqExponentialKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = expleft(-x-x^2right)","category":"page"},{"location":"kernels/#Gamma-Exponential-Kernel","page":"Kernel Functions","title":"Gamma Exponential Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The GammaExponentialKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxgamma) = expleft(-x-x^2gammaright)","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where gamma  0.","category":"page"},{"location":"kernels/#Exponentiated-Kernel","page":"Kernel Functions","title":"Exponentiated Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The ExponentiatedKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = expleft(langle xxrangle)","category":"page"},{"location":"kernels/#Fractional-Brownian-Motion","page":"Kernel Functions","title":"Fractional Brownian Motion","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The FBMKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxh) =  fracx^2h + x^2h - x-x^2h2","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where h is the Hurst index and 0h1.","category":"page"},{"location":"kernels/#Gabor-Kernel","page":"Kernel Functions","title":"Gabor Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The GaborKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx lp) = h(x-xlp)\n  h(ulp) = expleft(-cosleft(pi sum_i fracu_ip_iright)sum_i fracu_i^2l_i^2right)","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where l_i 0  is the lengthscale and p_i0 is the period.","category":"page"},{"location":"kernels/#Matern-Kernels","page":"Kernel Functions","title":"Matern Kernels","text":"","category":"section"},{"location":"kernels/#Matern-Kernel","page":"Kernel Functions","title":"Matern Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The MaternKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxnu) = frac2^1-nuGamma(nu)left(sqrt2nux-xright)K_nuleft(sqrt2nux-xright)","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where nu  0.","category":"page"},{"location":"kernels/#Matern-3/2-Kernel","page":"Kernel Functions","title":"Matern 3/2 Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The Matern32Kernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = left(1+sqrt3x-xright)expleft(sqrt3x-xright)","category":"page"},{"location":"kernels/#Matern-5/2-Kernel","page":"Kernel Functions","title":"Matern 5/2 Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The Matern52Kernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx) = left(1+sqrt5x-x+frac52x-x^2right)expleft(sqrt5x-xright)","category":"page"},{"location":"kernels/#Neural-Network-Kernel","page":"Kernel Functions","title":"Neural Network Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The NeuralNetworkKernel (as in the kernel for an infinitely wide neural network interpretated as a Gaussian process) is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(x x) = arcsinleft(fraclangle x xranglesqrt(1+langle x xrangle)(1+langle xxrangle)right)","category":"page"},{"location":"kernels/#Periodic-Kernel","page":"Kernel Functions","title":"Periodic Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The PeriodicKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxr) = expleft(-05 sum_i (sin (π(x_i - x_i))r_i)^2right)","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where r has the same dimension as x and r_i 0.","category":"page"},{"location":"kernels/#Piecewise-Polynomial-Kernel","page":"Kernel Functions","title":"Piecewise Polynomial Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The PiecewisePolynomialKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xx P V) = max(1 - r 0)^j + V f(r j)\n  r = x^top P x\n  j = lfloor fracD2rfloor + V + 1","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where xin mathbbR^D, V in 0123 and P is a positive definite matrix. f is a piecewise polynomial (see source code).","category":"page"},{"location":"kernels/#Polynomial-Kernels","page":"Kernel Functions","title":"Polynomial Kernels","text":"","category":"section"},{"location":"kernels/#Linear-Kernel","page":"Kernel Functions","title":"Linear Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The LinearKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxc) = langle xxrangle + c","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where c in mathbbR","category":"page"},{"location":"kernels/#Polynomial-Kernel","page":"Kernel Functions","title":"Polynomial Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The PolynomialKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxcd) = left(langle xxrangle + cright)^d","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where c in mathbbR and d0","category":"page"},{"location":"kernels/#Rational-Quadratic","page":"Kernel Functions","title":"Rational Quadratic","text":"","category":"section"},{"location":"kernels/#Rational-Quadratic-Kernel","page":"Kernel Functions","title":"Rational Quadratic Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The RationalQuadraticKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxalpha) = left(1+fracx-x^2alpharight)^-alpha","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where alpha  0.","category":"page"},{"location":"kernels/#Gamma-Rational-Quadratic-Kernel","page":"Kernel Functions","title":"Gamma Rational Quadratic Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The GammaRationalQuadraticKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxalphagamma) = left(1+fracx-x^2gammaalpharight)^-alpha","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where alpha  0 and gamma  0.","category":"page"},{"location":"kernels/#Spectral-Mixture-Kernel","page":"Kernel Functions","title":"Spectral Mixture Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The spectral mixture kernel is called by spectral_mixture_kernel.","category":"page"},{"location":"kernels/#Wiener-Kernel","page":"Kernel Functions","title":"Wiener Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The WienerKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"k(xxi) = leftbeginarraycc\n  delta(x x)  i = -1\n  min(xx)  i = 0\n  fracmin(xx)^2i+1a_i + b_i min(xx)^i+1x-xr_i(xx)  igeq 1\nendarrayright","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"where iin-10123 and coefficients a_i, b_i are fixed and residuals r_i are defined in the code.","category":"page"},{"location":"kernels/#Composite-Kernels","page":"Kernel Functions","title":"Composite Kernels","text":"","category":"section"},{"location":"kernels/#Transformed-Kernel","page":"Kernel Functions","title":"Transformed Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The TransformedKernel is a kernel where input are transformed via a function f","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxfwidetilek) = widetildek(f(x)f(x))","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"Where widetildek is another kernel and f is an arbitrary mapping.","category":"page"},{"location":"kernels/#Scaled-Kernel","page":"Kernel Functions","title":"Scaled Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The ScaledKernel is defined as","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxsigma^2widetildek) = sigma^2widetildek(xx)","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"Where widetildek is another kernel and sigma^2  0.","category":"page"},{"location":"kernels/#Kernel-Sum","page":"Kernel Functions","title":"Kernel Sum","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The KernelSum is defined as a sum of kernels","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(x x k_i) = sum_i k_i(x x)","category":"page"},{"location":"kernels/#KernelProduct","page":"Kernel Functions","title":"KernelProduct","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The KernelProduct is defined as a product of kernels","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxk_i) = prod_i k_i(xx)","category":"page"},{"location":"kernels/#Tensor-Product","page":"Kernel Functions","title":"Tensor Product","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The TensorProduct is defined as :","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  k(xxk_i) = prod_i k_i(x_ix_i)","category":"page"},{"location":"api/#API-Library","page":"API","title":"API Library","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/","page":"API","title":"API","text":"CurrentModule = KernelFunctions","category":"page"},{"location":"api/#Module","page":"API","title":"Module","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"KernelFunctions","category":"page"},{"location":"api/#KernelFunctions.KernelFunctions","page":"API","title":"KernelFunctions.KernelFunctions","text":"KernelFunctions. Github Documentation\n\n\n\n\n\n","category":"module"},{"location":"api/#Base-Kernels-API","page":"API","title":"Base Kernels API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"ConstantKernel\nWhiteKernel\nEyeKernel\nZeroKernel\nCosineKernel\nSqExponentialKernel\nExponentialKernel\nGammaExponentialKernel\nExponentiatedKernel\nFBMKernel\nGaborKernel\nMaternKernel\nMatern32Kernel\nMatern52Kernel\nNeuralNetworkKernel\nLinearKernel\nPolynomialKernel\nPiecewisePolynomialKernel\nMahalanobisKernel\nRationalQuadraticKernel\nGammaRationalQuadraticKernel\nspectral_mixture_kernel\nspectral_mixture_product_kernel\nPeriodicKernel\nWienerKernel","category":"page"},{"location":"api/#KernelFunctions.ConstantKernel","page":"API","title":"KernelFunctions.ConstantKernel","text":"ConstantKernel(; c=1.0)\n\nKernel function always returning a constant value c\n\n    κ(x,y) = c\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.WhiteKernel","page":"API","title":"KernelFunctions.WhiteKernel","text":"WhiteKernel()\n\n    κ(x,y) = δ(x,y)\n\nKernel function working as an equivalent to add white noise. Can also be called via EyeKernel()\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.EyeKernel","page":"API","title":"KernelFunctions.EyeKernel","text":"EyeKernel()\n\nSee WhiteKernel\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ZeroKernel","page":"API","title":"KernelFunctions.ZeroKernel","text":"ZeroKernel()\n\nCreate a kernel that always returning zero\n\n    κ(x,y) = 0.0\n\nThe output type depends of x and y\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.CosineKernel","page":"API","title":"KernelFunctions.CosineKernel","text":"CosineKernel()\n\nThe cosine kernel is a stationary kernel for a sinusoidal given by\n\n    κ(x,y) = cos( π * (x-y) )\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.SqExponentialKernel","page":"API","title":"KernelFunctions.SqExponentialKernel","text":"SqExponentialKernel()\n\nThe squared exponential kernel is a Mercer kernel given by the formula:\n\n    κ(x, y) = exp(-‖x - y‖² / 2)\n\nCan also be called via RBFKernel, GaussianKernel or SEKernel. See also ExponentialKernel for a related form of the kernel or GammaExponentialKernel for a generalization.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ExponentialKernel","page":"API","title":"KernelFunctions.ExponentialKernel","text":"ExponentialKernel()\n\nThe exponential kernel is a Mercer kernel given by the formula:\n\n    κ(x,y) = exp(-‖x-y‖)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.GammaExponentialKernel","page":"API","title":"KernelFunctions.GammaExponentialKernel","text":"GammaExponentialKernel(; γ = 2.0)\n\nThe γ-exponential kernel [1] is an isotropic Mercer kernel given by the formula:\n\n    κ(x,y) = exp(-‖x-y‖^γ)\n\nWhere γ > 0, (the keyword γ can be replaced by gamma) For γ = 2, see SqExponentialKernel and γ = 1, see ExponentialKernel.\n\n[1] - Gaussian Processes for Machine Learning, Carl Edward Rasmussen and Christopher K. I.     Williams, MIT Press, 2006.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ExponentiatedKernel","page":"API","title":"KernelFunctions.ExponentiatedKernel","text":"ExponentiatedKernel()\n\nThe exponentiated kernel is a Mercer kernel given by:\n\n    κ(x,y) = exp(xᵀy)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.FBMKernel","page":"API","title":"KernelFunctions.FBMKernel","text":"FBMKernel(; h::Real=0.5)\n\nFractional Brownian motion kernel with Hurst index h from (0,1) given by\n\n    κ(x,y) =  ( |x|²ʰ + |y|²ʰ - |x-y|²ʰ ) / 2\n\nFor h=1/2, this is the Wiener Kernel, for h>1/2, the increments are positively correlated and for h<1/2 the increments are negatively correlated.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.GaborKernel","page":"API","title":"KernelFunctions.GaborKernel","text":"GaborKernel(; ell::Real=1.0, p::Real=1.0)\n\nGabor kernel with lengthscale ell and period p. Given by\n\n    κ(xy) =  h(x-z) h(t) = exp(-sum(t^2(ell^2)))*cos(pi*sum(tp))\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.MaternKernel","page":"API","title":"KernelFunctions.MaternKernel","text":"MaternKernel(; ν = 1.0)\n\nThe matern kernel is a Mercer kernel given by the formula:\n\n    κ(x,y) = 2^{1-ν}/Γ(ν)*(√(2ν)‖x-y‖)^ν K_ν(√(2ν)‖x-y‖)\n\nFor ν=n+1/2, n=0,1,2,... it can be simplified and you should instead use  ExponentialKernel for n=0, Matern32Kernel, for n=1,  Matern52Kernel for n=2 and SqExponentialKernel for n=∞.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.Matern32Kernel","page":"API","title":"KernelFunctions.Matern32Kernel","text":"Matern32Kernel()\n\nThe matern 3/2 kernel is a Mercer kernel given by the formula:\n\n    κ(x,y) = (1+√(3)‖x-y‖)exp(-√(3)‖x-y‖)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.Matern52Kernel","page":"API","title":"KernelFunctions.Matern52Kernel","text":"Matern52Kernel()\n\nThe matern 5/2 kernel is a Mercer kernel given by the formula:\n\n    κ(x,y) = (1+√(5)‖x-y‖ + 5/3‖x-y‖^2)exp(-√(5)‖x-y‖)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.NeuralNetworkKernel","page":"API","title":"KernelFunctions.NeuralNetworkKernel","text":"NeuralNetworkKernel()\n\nNeural network kernel function.\n\n    κ(x y) =  asin(x * y  sqrt(1 + x * x) * (1 + y * y))\n\nSignificance\n\nNeal (1996) pursued the limits of large models, and showed that a Bayesian neural network becomes a Gaussian process with a neural network kernel as the number of units approaches infinity. Here, we give the neural network kernel for single hidden layer Bayesian neural network with erf (Error Function) as activation function.\n\nReferences:\n\nGPML Pg 105\nNeal(1996)\nAndrew Gordon's Thesis Pg 45\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.LinearKernel","page":"API","title":"KernelFunctions.LinearKernel","text":"LinearKernel(; c = 0.0)\n\nThe linear kernel is a Mercer kernel given by\n\n    κ(x,y) = xᵀy + c\n\nWhere c is a real number\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.PolynomialKernel","page":"API","title":"KernelFunctions.PolynomialKernel","text":"PolynomialKernel(; d = 2.0, c = 0.0)\n\nThe polynomial kernel is a Mercer kernel given by\n\n    κ(x,y) = (xᵀy + c)^d\n\nWhere c is a real number, and d is a shape parameter bigger than 1. For d = 1 see LinearKernel\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.PiecewisePolynomialKernel","page":"API","title":"KernelFunctions.PiecewisePolynomialKernel","text":"PiecewisePolynomialKernel{V}(maha::AbstractMatrix)\n\nPiecewise Polynomial covariance function with compact support, V = 0,1,2,3. The kernel functions are 2v times continuously differentiable and the corresponding processes are hence v times  mean-square differentiable. The kernel function is:\n\n    κ(x y) = max(1 - r 0)^(j + V) * f(r j) with j = floor(D  2) + V + 1\n\nwhere r is the Mahalanobis distance mahalanobis(x,y) with maha as the metric.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.MahalanobisKernel","page":"API","title":"KernelFunctions.MahalanobisKernel","text":"MahalanobisKernel(P::AbstractMatrix)\n\nMahalanobis distance-based kernel given by\n\n    κ(xy) =  exp(-r^2) r^2 = maha(xPy) = (x-y)* P *(x-y)\n\nwhere the matrix P is the metric.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.RationalQuadraticKernel","page":"API","title":"KernelFunctions.RationalQuadraticKernel","text":"RationalQuadraticKernel(; α=2.0)\n\nThe rational-quadratic kernel is a Mercer kernel given by the formula:\n\n    κ(x, y) = (1 + ||x − y||² / (2α))^(-α)\n\nwhere α is a shape parameter of the Euclidean distance. Check GammaRationalQuadraticKernel for a generalization.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.GammaRationalQuadraticKernel","page":"API","title":"KernelFunctions.GammaRationalQuadraticKernel","text":"GammaRationalQuadraticKernel([α=2.0 [, γ=2.0]])\n\nThe Gamma-rational-quadratic kernel is an isotropic Mercer kernel given by the formula:\n\n    κ(x, y) = (1 + ||x−y||^γ / α)^(-α)\n\nwhere α is a shape parameter of the Euclidean distance and γ is another shape parameter.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.spectral_mixture_kernel","page":"API","title":"KernelFunctions.spectral_mixture_kernel","text":"spectral_mixture_kernel(\n    h::Kernel=SqExponentialKernel(),\n    αs::AbstractVector{<:Real},\n    γs::AbstractMatrix{<:Real},\n    ωs::AbstractMatrix{<:Real},\n)\n\nwhere αs are the weights of dimension (A, ), γs is the covariance matrix of dimension (D, A) and ωs are the mean vectors and is of dimension (D, A). Here, D is input dimension and A is the number of spectral components.\n\nh is the kernel, which defaults to SqExponentialKernel if not specified.\n\nGeneralised Spectral Mixture kernel function. This family of functions is  dense in the family of stationary real-valued kernels with respect to the pointwise convergence.[1]\n\n   κ(x y) = αs (h(-(γs * t)^2) * cos(π * ωs * t) t = x - y\n\nReferences:\n\n[1] Generalized Spectral Kernels, by Yves-Laurent Kom Samo and Stephen J. Roberts\n[2] SM: Gaussian Process Kernels for Pattern Discovery and Extrapolation,\n        ICML, 2013, by Andrew Gordon Wilson and Ryan Prescott Adams,\n[3] Covariance kernels for fast automatic pattern discovery and extrapolation\n    with Gaussian processes, Andrew Gordon Wilson, PhD Thesis, January 2014.\n    http://www.cs.cmu.edu/~andrewgw/andrewgwthesis.pdf\n[4] http://www.cs.cmu.edu/~andrewgw/pattern/.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.spectral_mixture_product_kernel","page":"API","title":"KernelFunctions.spectral_mixture_product_kernel","text":"spectral_mixture_product_kernel(\n    h::Kernel=SqExponentialKernel(),\n    αs::AbstractMatrix{<:Real},\n    γs::AbstractMatrix{<:Real},\n    ωs::AbstractMatrix{<:Real},\n)\n\nwhere αs are the weights of dimension (D, A), γs is the covariance matrix of dimension (D, A) and ωs are the mean vectors and is of dimension (D, A). Here, D is input dimension and A is the number of spectral components.\n\nSpectral Mixture Product Kernel. With enough components A, the SMP kernel can model any product kernel to arbitrary precision, and is flexible even with a small number of components [1]\n\nh is the kernel, which defaults to SqExponentialKernel if not specified.\n\n   κ(x y) = Πᵢ₁ᴷ Σ(αsᵢᵀ * (h(-(γsᵢᵀ * tᵢ)²) * cos(ωsᵢᵀ * tᵢ))) tᵢ = xᵢ - yᵢ\n\nReferences:\n\n[1] GPatt: Fast Multidimensional Pattern Extrapolation with GPs,\n    arXiv 1310.5288, 2013, by Andrew Gordon Wilson, Elad Gilboa,\n    Arye Nehorai and John P. Cunningham\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.PeriodicKernel","page":"API","title":"KernelFunctions.PeriodicKernel","text":"PeriodicKernel(r::AbstractVector)\nPeriodicKernel(dims::Int)\nPeriodicKernel(T::DataType, dims::Int)\n\nPeriodic Kernel as described in http://www.inference.org.uk/mackay/gpB.pdf eq. 47.\n\n    κ(x,y) = exp( - 0.5 sum_i(sin (π(x_i - y_i))/r_i))\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.WienerKernel","page":"API","title":"KernelFunctions.WienerKernel","text":"WienerKernel{i}()\n\ni-times integrated Wiener process kernel function.\n\nFor i=-1, this is just the white noise covariance, see WhiteKernel.\nFor i= 0, this is the Wiener process covariance,\nFor i= 1, this is the integrated Wiener process covariance (velocity),\nFor i= 2, this is the twice-integrated Wiener process covariance (accel.),\nFor i= 3, this is the thrice-integrated Wiener process covariance,\n\nwhere κᵢ is given by\n\n    κ₁(x y) =  δ(x y)\n    κ₀(x y)  =  min(x y)\n\nand for i = 1,\n\n    κᵢ(x y) = 1  aᵢ * min(x y)^(2i + 1) + bᵢ * min(x y)^(i + 1) * x - y * rᵢ(x y)\n\nwith the coefficients ``aᵢ``, ``bᵢ`` and the residual ``rᵢ(x, y)`` defined as follows:\n\n    a₁ = 3 b₁ = 12 r₁(x y) = 1\n    a₂ = 20 b₂ = 112 r₂(x y) = x + y - min(x y)  2\n    a₃ = 252 b₃ = 1720 r₃(x y) = 5 * max(x y)² + 2 * x * z + 3 * min(x y)²\n\n\nReferences:\n\nSee the paper Probabilistic ODE Solvers with Runge-Kutta Means by Schober, Duvenaud and Hennig, NIPS, 2014, for more details.\n\n\n\n\n\n","category":"type"},{"location":"api/#Composite-Kernels","page":"API","title":"Composite Kernels","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"TransformedKernel\nScaledKernel\nKernelSum\nKernelProduct\nTensorProduct","category":"page"},{"location":"api/#KernelFunctions.TransformedKernel","page":"API","title":"KernelFunctions.TransformedKernel","text":"TransformedKernel(k::Kernel,t::Transform)\n\nReturn a kernel where inputs are pretransformed by t : k(t(x),t(x')) Can also be called via transform : transform(k, t)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ScaledKernel","page":"API","title":"KernelFunctions.ScaledKernel","text":"ScaledKernel(k::Kernel, σ²::Real)\n\nReturn a kernel premultiplied by the variance σ² : σ² k(x,x')\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.KernelSum","page":"API","title":"KernelFunctions.KernelSum","text":"KernelSum <: Kernel\n\nCreate a sum of kernels. One can also use the operator +.\n\nThere are various ways in which you create a KernelSum:\n\nThe simplest way to specify a KernelSum would be to use the overloaded + operator. This is  equivalent to creating a KernelSum by specifying the kernels as the arguments to the constructor.  \n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5);\n\njulia> (k = k1 + k2) == KernelSum(k1, k2)\ntrue\n\njulia> kernelmatrix(k1 + k2, X) == kernelmatrix(k1, X) .+ kernelmatrix(k2, X)\ntrue\n\njulia> kernelmatrix(k, X) == kernelmatrix(k1 + k2, X)\ntrue\n\nYou could also specify a KernelSum by providing a Tuple or a Vector of the  kernels to be summed. We suggest you to use a Tuple when you have fewer components   and a Vector when dealing with a large number of components.\n\njulia> KernelSum((k1, k2)) == k1 + k2\ntrue\n\njulia> KernelSum([k1, k2]) == KernelSum((k1, k2)) == k1 + k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.KernelProduct","page":"API","title":"KernelFunctions.KernelProduct","text":"KernelProduct <: Kernel\n\nCreate a product of kernels. One can also use the overloaded operator *.\n\nThere are various ways in which you create a KernelProduct:\n\nThe simplest way to specify a KernelProduct would be to use the overloaded * operator. This is  equivalent to creating a KernelProduct by specifying the kernels as the arguments to the constructor.  \n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5);\n\njulia> (k = k1 * k2) == KernelProduct(k1, k2)\ntrue\n\njulia> kernelmatrix(k1 * k2, X) == kernelmatrix(k1, X) .* kernelmatrix(k2, X)\ntrue\n\njulia> kernelmatrix(k, X) == kernelmatrix(k1 * k2, X)\ntrue\n\nYou could also specify a KernelProduct by providing a Tuple or a Vector of the  kernels to be multiplied. We suggest you to use a Tuple when you have fewer components   and a Vector when dealing with a large number of components.\n\njulia> KernelProduct((k1, k2)) == k1 * k2\ntrue\n\njulia> KernelProduct([k1, k2]) == KernelProduct((k1, k2)) == k1 * k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.TensorProduct","page":"API","title":"KernelFunctions.TensorProduct","text":"TensorProduct(kernels...)\n\nCreate a tensor product kernel from kernels k_1 ldots k_n, i.e., a kernel k that is given by\n\nk(x y) = prod_i=1^n k_i(x_i y_i)\n\nThe kernels can be specified as individual arguments, a tuple, or an iterable data structure such as an array. Using a tuple or individual arguments guarantees that TensorProduct is concretely typed but might lead to large compilation times if the number of kernels is large.\n\n\n\n\n\n","category":"type"},{"location":"api/#Transforms","page":"API","title":"Transforms","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Transform\nIdentityTransform\nScaleTransform\nARDTransform\nLinearTransform\nFunctionTransform\nSelectTransform\nChainTransform","category":"page"},{"location":"api/#KernelFunctions.Transform","page":"API","title":"KernelFunctions.Transform","text":"Abstract type defining a slice-wise transformation on an input matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.IdentityTransform","page":"API","title":"KernelFunctions.IdentityTransform","text":"IdentityTransform()\n\nReturn exactly the input\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ScaleTransform","page":"API","title":"KernelFunctions.ScaleTransform","text":"ScaleTransform(l::Real)\n\nMultiply every element of the input by l\n\n    l = 2.0\n    tr = ScaleTransform(l)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ARDTransform","page":"API","title":"KernelFunctions.ARDTransform","text":"ARDTransform(v::AbstractVector)\nARDTransform(s::Real, dims::Int)\n\nMultiply every vector of observation by v element-wise\n\n    v = rand(3)\n    tr = ARDTransform(v)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.LinearTransform","page":"API","title":"KernelFunctions.LinearTransform","text":"LinearTransform(A::AbstractMatrix)\n\nApply the linear transformation realised by the matrix A.\n\nThe second dimension of A must match the number of features of the target.\n\nExamples\n\njulia> A = rand(10, 5)\n\njulia> tr = LinearTransform(A)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.FunctionTransform","page":"API","title":"KernelFunctions.FunctionTransform","text":"FunctionTransform(f)\n\nTake a function or object f as an argument which is going to act on each vector individually. Make sure that f is supposed to act on a vector. For example replace f(x)=sin(x) by f(x)=sin.(x)\n\n    f(x) = abs.(x)\n    tr = FunctionTransform(f)\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.SelectTransform","page":"API","title":"KernelFunctions.SelectTransform","text":"SelectTransform(dims)\n\nSelect the dimensions dims that the kernel is applied to.\n\n    dims = [1,3,5,6,7]\n    tr = SelectTransform(dims)\n    X = rand(100,10)\n    transform(tr,X,obsdim=2) == X[dims,:]\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.ChainTransform","page":"API","title":"KernelFunctions.ChainTransform","text":"ChainTransform(ts::AbstractVector{<:Transform})\n\nChain a series of transform, here t1 will be called first\n\n    t1 = ScaleTransform()\n    t2 = LinearTransform(rand(3,4))\n    ct = ChainTransform([t1,t2]) #t1 will be called first\n    ct == t2 ∘ t1\n\n\n\n\n\n","category":"type"},{"location":"api/#Functions","page":"API","title":"Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"kernelmatrix\nkernelmatrix!\nkerneldiagmatrix\nkerneldiagmatrix!\nkernelpdmat\nkernelkronmat\nnystrom\ntransform","category":"page"},{"location":"api/#KernelFunctions.kernelmatrix","page":"API","title":"KernelFunctions.kernelmatrix","text":"kernelmatrix(κ::Kernel, X; obsdim::Int = 2)\nkernelmatrix(κ::Kernel, X, Y; obsdim::Int = 2)\n\nCalculate the kernel matrix of X (and Y) with respect to kernel κ. obsdim = 1 means the matrix X (and Y) has size #samples x #dimension obsdim = 2 means the matrix X (and Y) has size #dimension x #samples\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kernelmatrix!","page":"API","title":"KernelFunctions.kernelmatrix!","text":"kernelmatrix!(K::AbstractMatrix, κ::Kernel, X; obsdim::Integer = 2)\nkernelmatrix!(K::AbstractMatrix, κ::Kernel, X, Y; obsdim::Integer = 2)\n\nIn-place version of kernelmatrix where pre-allocated matrix K will be overwritten with the kernel matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kerneldiagmatrix","page":"API","title":"KernelFunctions.kerneldiagmatrix","text":"kerneldiagmatrix(κ::Kernel, X; obsdim::Int = 2)\n\nCalculate the diagonal matrix of X with respect to kernel κ obsdim = 1 means the matrix X has size #samples x #dimension obsdim = 2 means the matrix X has size #dimension x #samples\n\nkerneldiagmatrix(κ::Kernel, X, Y; obsdim::Int = 2)\n\nCalculate the diagonal of kernelmatrix(κ, X, Y; obsdim) efficiently. Requires that X and Y are the same length.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kerneldiagmatrix!","page":"API","title":"KernelFunctions.kerneldiagmatrix!","text":"kerneldiagmatrix!(K::AbstractVector, κ::Kernel, X; obsdim::Int = 2)\nkerneldiagmatrix!(K::AbstractVector, κ::Kernel, X, Y; obsdim::Int = 2)\n\nIn place version of kerneldiagmatrix\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.nystrom","page":"API","title":"KernelFunctions.nystrom","text":"nystrom(k::Kernel, X::Matrix, S::Vector; obsdim::Int=defaultobs)\n\nComputes a factorization of Nystrom approximation of the square kernel matrix of data matrix X with respect to kernel k. Returns a NystromFact struct which stores a Nystrom factorization satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\nnystrom(k::Kernel, X::Matrix, r::Real; obsdim::Int=defaultobs)\n\nComputes a factorization of Nystrom approximation of the square kernel matrix of data matrix X with respect to kernel k using a sample ratio of r. Returns a NystromFact struct which stores a Nystrom factorization satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.transform","page":"API","title":"KernelFunctions.transform","text":"    transform(k::Kernel, t::Transform) (1)\n    transform(k::Kernel, ρ::Real) (2)\n    transform(k::Kernel, ρ::AbstractVector) (3)\n\n(1) Create a TransformedKernel with transform t and kernel k (2) Same as (1) with a ScaleTransform with scale ρ (3) Same as (1) with an ARDTransform with scales ρ\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"ColVecs\nRowVecs\nNystromFact","category":"page"},{"location":"api/#KernelFunctions.ColVecs","page":"API","title":"KernelFunctions.ColVecs","text":"ColVecs(X::AbstractMatrix)\n\nA lightweight wrapper for an AbstractMatrix to make it behave like a vector of vectors. Each vector represents a column of the matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.RowVecs","page":"API","title":"KernelFunctions.RowVecs","text":"RowVecs(X::AbstractMatrix)\n\nA lightweight wrapper for an AbstractMatrix to make it behave like a vector of vectors. Each vector represents a row of the matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.NystromFact","page":"API","title":"KernelFunctions.NystromFact","text":"NystromFact\n\nType for storing a Nystrom factorization. The factorization contains two fields: W and C, two matrices satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\n","category":"type"},{"location":"api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]\nModule = [\"KernelFunctions\"]\nOrder = [:type, :function]","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"See Wikipedia article","category":"page"},{"location":"create_kernel/#Creating-your-own-kernel","page":"Custom Kernels","title":"Creating your own kernel","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions.jl contains the most popular kernels already but you might want to make your own!","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Here are a few ways depending on how complicated your kernel is :","category":"page"},{"location":"create_kernel/#SimpleKernel-for-kernels-function-depending-on-a-metric","page":"Custom Kernels","title":"SimpleKernel for kernels function depending on a metric","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If your kernel function is of the form k(x, y) = f(d(x, y)) where d(x, y) is a PreMetric, you can construct your custom kernel by defining kappa and metric for your kernel. Here is for example how one can define the SqExponentialKernel again :","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"struct MyKernel <: KernelFunctions.SimpleKernel end\n\nKernelFunctions.kappa(::MyKernel, d2::Real) = exp(-d2)\nKernelFunctions.metric(::MyKernel) = SqEuclidean()","category":"page"},{"location":"create_kernel/#Kernel-for-more-complex-kernels","page":"Custom Kernels","title":"Kernel for more complex kernels","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If your kernel does not satisfy such a representation, all you need to do is define (k::MyKernel)(x, y) and inherit from Kernel. For example we recreate here the NeuralNetworkKernel","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"struct MyKernel <: KernelFunctions.Kernel end\n\n(::MyKernel)(x, y) = asin(dot(x, y) / sqrt((1 + sum(abs2, x)) * (1 + sum(abs2, y))))","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Note that BaseKernel do not use Distances.jl and can therefore be a bit slower.","category":"page"},{"location":"create_kernel/#Additional-Options","page":"Custom Kernels","title":"Additional Options","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Finally there are additional functions you can define to bring in more features:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions.iskroncompatible(k::MyKernel): if your kernel factorizes in dimensions, you can declare your kernel as iskroncompatible(k) = true to use Kronecker methods.\nKernelFunctions.dim(x::MyDataType): by default the dimension of the inputs will only be checked for vectors of type AbstractVector{<:Real}. If you want to check the dimensionality of your inputs, dispatch the dim function on your datatype. Note that 0 is the default.\ndim is called within KernelFunctions.validate_inputs(x::MyDataType, y::MyDataType), which can instead be directly overloaded if you want to run special checks for your input types.\nkernelmatrix(k::MyKernel, ...): you can redefine the diverse kernelmatrix functions to eventually optimize the computations.\nBase.print(io::IO, k::MyKernel): if you want to specialize the printing of your kernel","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions uses Functors.jl for specifying trainable kernel parameters in a way that is compatible with the Flux ML framework. You can use Functors.@functor if all fields of your kernel struct are trainable. Note that optimization algorithms in Flux are not compatible with scalar parameters (yet), and hence vector-valued parameters should be preferred.","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"import Functors\n\nstruct MyKernel{T} <: KernelFunctions.Kernel\n    a::Vector{T}\nend\n\nFunctors.@functor MyKernel","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If only a subset of the fields are trainable, you have to specify explicitly how to (re)construct the kernel with modified parameter values by implementing Functors.functor(::Type{<:MyKernel}, x) for your kernel struct:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"import Functors\n\nstruct MyKernel{T} <: KernelFunctions.Kernel\n    n::Int\n    a::Vector{T}\nend\n\nfunction Functors.functor(::Type{<:MyKernel}, x::MyKernel)\n    function reconstruct_mykernel(xs)\n        # keep field `n` of the original kernel and set `a` to (possibly different) `xs.a`\n        return MyKernel(x.n, xs.a)\n    end\n    return (a = x.a,), reconstruct_mykernel\nend","category":"page"},{"location":"transform/#Transform","page":"Transform","title":"Transform","text":"","category":"section"},{"location":"transform/","page":"Transform","title":"Transform","text":"Transform is the object that takes care of transforming the input data before distances are being computed. It can be as standard as IdentityTransform returning the same input, or multiplying the data by a scalar with ScaleTransform or by a vector with ARDTransform. There is a more general Transform: FunctionTransform that uses a function and apply it on each vector via mapslices. You can also create a pipeline of Transform via TransformChain. For example LowRankTransform(rand(10,5))∘ScaleTransform(2.0).","category":"page"},{"location":"transform/","page":"Transform","title":"Transform","text":"One apply a transformation on a matrix or a vector via KernelFunctions.apply(t::Transform,v::AbstractVecOrMat)","category":"page"},{"location":"transform/","page":"Transform","title":"Transform","text":"Check the list on the API page","category":"page"},{"location":"userguide/#User-guide","page":"User Guide","title":"User guide","text":"","category":"section"},{"location":"userguide/#Kernel-creation","page":"User Guide","title":"Kernel creation","text":"","category":"section"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"To create a kernel chose one of the kernels proposed, see Base Kernels, or create your own, see Creating your own kernel For example to create a square exponential kernel","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  k = SqExponentialKernel()","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"Instead of having lengthscale(s) for each kernel we use Transform objects (see Transform) which are directly going to act on the inputs before passing them to the kernel. For example to premultiply the input by 2.0 we create the kernel the following options are possible","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  k = transform(SqExponentialKernel(),ScaleTransform(2.0)) # returns a TransformedKernel\n  k = @kernel SqExponentialKernel() l=2.0 # Will be available soon\n  k = TransformedKernel(SqExponentialKernel(),ScaleTransform(2.0))","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"Check the Transform page to see the other options. To premultiply the kernel by a variance, you can use * or create a ScaledKernel","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  k = 3.0*SqExponentialKernel()\n  k = ScaledKernel(SqExponentialKernel(),3.0)\n  @kernel 3.0*SqExponentialKernel()","category":"page"},{"location":"userguide/#Using-a-kernel-function","page":"User Guide","title":"Using a kernel function","text":"","category":"section"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"To compute the kernel function on two vectors you can call","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  k = SqExponentialKernel()\n  x1 = rand(3)\n  x2 = rand(3)\n  k(x1,x2)","category":"page"},{"location":"userguide/#Creating-a-kernel-matrix","page":"User Guide","title":"Creating a kernel matrix","text":"","category":"section"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"Kernel matrices can be created via the kernelmatrix function or kerneldiagmatrix for only the diagonal. An important argument to give is the dimensionality of the input obsdim. It tells if the matrix is of the type # samples X # features (obsdim=1) or # features X # samples(obsdim=2) (similarly to Distances.jl) For example:","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  k = SqExponentialKernel()\n  A = rand(10,5)\n  kernelmatrix(k,A,obsdim=1) # Return a 10x10 matrix\n  kernelmatrix(k,A,obsdim=2) # Return a 5x5 matrix\n  k(A,obsdim=1) # Syntactic sugar","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"We also support specific kernel matrices outputs:","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"For a positive-definite matrix objectPDMat from PDMats.jl, you can call the following:","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  using PDMats\n  k = SqExponentialKernel()\n  K = kernelpdmat(k,A,obsdim=1) # PDMat","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"It will create a matrix and in case of bad conditionning will add some diagonal noise until the matrix is considered PSD, it will then return a PDMat object. For this method to work in your code you need to include using PDMats first","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"For a Kronecker matrix, we rely on Kronecker.jl. Here are two examples:","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"using Kronecker\nx = range(0,1,length=10)\ny = range(0,1,length=50)\nK = kernelkronmat(k,[x,y]) # Kronecker matrix\nK = kernelkronmat(k,x,5) # Kronecker matrix","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"Make sure that k is a vector compatible with such constructions (with iskroncompatible). Both method will return a . For those methods to work in your code you need to include using Kronecker first","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"For a Nystrom approximation : kernelmatrix(nystrom(k, X, ρ, obsdim = 1)) where ρ is the proportion of sampled used.","category":"page"},{"location":"userguide/#Composite-kernels","page":"User Guide","title":"Composite kernels","text":"","category":"section"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"One can create combinations of kernels via KernelSum and KernelProduct or using simple operators + and *. For example :","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  k1 = SqExponentialKernel()\n  k2 = Matern32Kernel()\n  k = 0.5 * k1 + 0.2 * k2 # KernelSum\n  k = k1 * k2 # KernelProduct","category":"page"},{"location":"userguide/#Kernel-Parameters","page":"User Guide","title":"Kernel Parameters","text":"","category":"section"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"What if you want to differentiate through the kernel parameters? Even in a highly nested structure such as :","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  k = transform(0.5*SqExponentialKernel()*MaternKernel()+0.2*(transform(LinearKernel(),2.0)+PolynomialKernel()),[0.1,0.5])","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"One can get the array of parameters to optimize via params from Flux.jl","category":"page"},{"location":"userguide/","page":"User Guide","title":"User Guide","text":"  using Flux\n  params(k)","category":"page"},{"location":"#KernelFunctions.jl","page":"Home","title":"KernelFunctions.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Model agnostic kernel functions compatible with automatic differentiation","category":"page"},{"location":"","page":"Home","title":"Home","text":"KernelFunctions.jl is a general purpose kernel package. It aims at providing a flexible framework for creating kernels and manipulating them. The main goals of this package compared to its predecessors/concurrents in MLKernels.jl, Stheno.jl, GaussianProcesses.jl and AugmentedGaussianProcesses.jl are:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Automatic Differentation compatibility: all kernel functions should be differentiable via packages like ForwardDiff.jl or Zygote.jl\nFlexibility: operations between kernels should be fluid and easy without breaking.\nPlug-and-play: including the kernels before/after other steps should be straightforward.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The methodology of how kernels are computed is quite simple and is done in three phases :","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Transform object is applied sample-wise on every sample\nThe pairwise matrix is computed using Distances.jl by using a Metric proper to each kernel\nThe Kernel function is applied element-wise on the pairwise matrix","category":"page"},{"location":"","page":"Home","title":"Home","text":"For a quick introduction on how to use it go to User guide","category":"page"},{"location":"example/#Examples-(WIP)","page":"Examples","title":"Examples (WIP)","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Here are a few examples of known complex kernels and how to do them. Or how to use kernels in a certain context","category":"page"},{"location":"example/#Kernel-Ridge-Regression","page":"Examples","title":"Kernel Ridge Regression","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Make a simple example of kernel ridge regression","category":"page"},{"location":"example/#Gaussian-Process-Regression","page":"Examples","title":"Gaussian Process Regression","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Make a simple example of gaussian process regression","category":"page"},{"location":"example/#Deep-Kernel-Learning","page":"Examples","title":"Deep Kernel Learning","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Put a Flux neural net in front of the kernel cf. Wilson paper","category":"page"},{"location":"example/#Kernel-Selection","page":"Examples","title":"Kernel Selection","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Create a large collection of kernels and optimize the weights cf AISTATS 2018 paper","category":"page"}]
}
