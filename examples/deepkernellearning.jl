# # Deep Kernel Learning with Flux
# ## Package loading
# We use a couple of useful packages to plot and optimize
# the different hyper-parameters
using KernelFunctions
using Flux
using Distributions, LinearAlgebra
using Plots
using AbstractGPs
pyplot(); default(legendfontsize = 15.0, linewidth = 3.0)
# using SliceMap



# Base.map(
#     t::KernelFunctions.FunctionTransform,
#     X::AbstractVector;
#     obsdim::Int = defaultobs,
# ) where {T} =
#     slicemap(X, dims = 1) do x
#         t.f(x)
#     end

# ## Data creation
# We create a simple 1D Problem with very different variations
xmin = -3; xmax = 3 # Limits
N = 100
noise = 0.01
x_train = collect(eachrow(rand(Uniform(xmin, xmax), N))) # Training dataset
target_f(x) = sinc(abs(x) ^ abs(x)) # We use sinc with a highly varying value
target_f(x::AbstractArray) = target_f(first(x))
y_train = target_f.(x_train) + randn(N) * noise
x_test = collect(eachrow(range(xmin, xmax, length=200))) # Testing dataset

# ## Model definition
# We create a neural net with 2 layers and 10 units each
# The data is passed through the NN before being used in the kernel
neuralnet = Chain(Dense(1, 10), Dense(10, 2))
# We use a Squared Exponential Kernel
k = transform(SqExponentialKernel(), FunctionTransform(neuralnet))

# We use AbstractGPs.jl to define our model
gpprior = GP(k) # GP Prior
fx = AbstractGPs.FiniteGP(gpprior, x_train, noise) # Prior on f
fp = posterior(fx, y_train) # Posterior of f

# This compute the log evidence of `y`,
# which is going to be used as the objective
loss(y) = logpdf(fx, y)

@info "Init Loss = $(loss(y_train))"

# Flux will automatically extract all the parameters of the kernel
ps = Flux.params(k)

# We show the initial prediction with the untrained model
p = Plots.plot(vcat(x_test...), target_f, lab = "true f", title = "Loss = $(loss(y_train))")
Plots.scatter!(vcat(x_train...), y_train, lab = "data")
pred = marginals(fp(x_test))
Plots.plot!(vcat(x_test...), mean.(pred), ribbon = std.(pred), lab = "Prediction")
# ## Training
anim = Animation()
nmax= 10
opt = Flux.ADAGrad(0.001)
for i = 1:nmax
    grads = gradient(ps) do
        loss(y_train)
    end
    Flux.Optimise.update!(opt, ps, grads)
    if i % 100 == 0
        @info "$i/$nmax"
        p = Plots.plot(vcat(x_test...), target_f, lab = "true f", title = "Loss = $(loss(y_train))")
        p = Plots.scatter!(vcat(x_train...), y_train, lab = "data")
        pred = marginals(fp(x_test))
        Plots.plot!(vcat(x_test...), mean.(pred), ribbon = std.(pred), lab = "Prediction")
        frame(anim)
    end
end
gif(anim, fps = 5)
