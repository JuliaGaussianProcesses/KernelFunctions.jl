var documenterSearchIndex = {"docs":
[{"location":"metrics/#Metrics","page":"Metrics","title":"Metrics","text":"","category":"section"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"SimpleKernel implementations rely on Distances.jl for efficiently computing the pairwise matrix. This requires a distance measure or metric, such as the commonly used SqEuclidean and Euclidean.","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"The metric used by a given kernel type is specified as","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"KernelFunctions.metric(::CustomKernel) = SqEuclidean()","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"However, there are kernels that can be implemented efficiently using \"metrics\" that do not respect all the definitions expected by Distances.jl. For this reason, KernelFunctions.jl provides additional \"metrics\" such as DotProduct (langle x y rangle) and Delta (delta(xy)).","category":"page"},{"location":"metrics/#Adding-a-new-metric","page":"Metrics","title":"Adding a new metric","text":"","category":"section"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"If you want to create a new \"metric\" just implement the following:","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"struct Delta <: Distances.PreMetric\nend\n\n@inline function Distances._evaluate(::Delta,a::AbstractVector{T},b::AbstractVector{T}) where {T}\n    @boundscheck if length(a) != length(b)\n        throw(DimensionMismatch(\"first array has length $(length(a)) which does not match the length of the second, $(length(b)).\"))\n    end\n    return a==b\nend\n\n@inline (dist::Delta)(a::AbstractArray,b::AbstractArray) = Distances._evaluate(dist,a,b)\n@inline (dist::Delta)(a::Number,b::Number) = a==b","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  CurrentModule = KernelFunctions","category":"page"},{"location":"kernels/#Kernel-Functions","page":"Kernel Functions","title":"Kernel Functions","text":"","category":"section"},{"location":"kernels/#base_kernels","page":"Kernel Functions","title":"Base Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"These are the basic kernels without any transformation of the data. They are the building blocks of KernelFunctions.","category":"page"},{"location":"kernels/#Constant-Kernels","page":"Kernel Functions","title":"Constant Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"ZeroKernel\nConstantKernel\nWhiteKernel\nEyeKernel","category":"page"},{"location":"kernels/#KernelFunctions.ZeroKernel","page":"Kernel Functions","title":"KernelFunctions.ZeroKernel","text":"ZeroKernel()\n\nZero kernel.\n\nDefinition\n\nFor inputs x x, the zero kernel is defined as\n\nk(x x) = 0\n\nThe output type depends on x and x.\n\nSee also: ConstantKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.ConstantKernel","page":"Kernel Functions","title":"KernelFunctions.ConstantKernel","text":"ConstantKernel(; c::Real=1.0)\n\nKernel of constant value c.\n\nDefinition\n\nFor inputs x x, the kernel of constant value c geq 0 is defined as\n\nk(x x) = c\n\nSee also: ZeroKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.WhiteKernel","page":"Kernel Functions","title":"KernelFunctions.WhiteKernel","text":"WhiteKernel()\n\nWhite noise kernel.\n\nDefinition\n\nFor inputs x x, the white noise kernel is defined as\n\nk(x x) = delta(x x)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.EyeKernel","page":"Kernel Functions","title":"KernelFunctions.EyeKernel","text":"EyeKernel()\n\nAlias of WhiteKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Cosine-Kernel","page":"Kernel Functions","title":"Cosine Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"CosineKernel","category":"page"},{"location":"kernels/#KernelFunctions.CosineKernel","page":"Kernel Functions","title":"KernelFunctions.CosineKernel","text":"CosineKernel()\n\nCosine kernel.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the cosine kernel is defined as\n\nk(x x) = cos(pi x-x_2)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Exponential-Kernels","page":"Kernel Functions","title":"Exponential Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"ExponentialKernel\nLaplacianKernel\nSqExponentialKernel\nSEKernel\nGaussianKernel\nRBFKernel\nGammaExponentialKernel","category":"page"},{"location":"kernels/#KernelFunctions.ExponentialKernel","page":"Kernel Functions","title":"KernelFunctions.ExponentialKernel","text":"ExponentialKernel()\n\nExponential kernel.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the exponential kernel is defined as\n\nk(x x) = expbig(- x - x_2big)\n\nSee also: GammaExponentialKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.LaplacianKernel","page":"Kernel Functions","title":"KernelFunctions.LaplacianKernel","text":"LaplacianKernel()\n\nAlias of ExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.SqExponentialKernel","page":"Kernel Functions","title":"KernelFunctions.SqExponentialKernel","text":"SqExponentialKernel()\n\nSquared exponential kernel.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the squared exponential kernel is defined as\n\nk(x x) = expbigg(- fracx - x_2^22bigg)\n\nSee also: GammaExponentialKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.SEKernel","page":"Kernel Functions","title":"KernelFunctions.SEKernel","text":"SEKernel()\n\nAlias of SqExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.GaussianKernel","page":"Kernel Functions","title":"KernelFunctions.GaussianKernel","text":"GaussianKernel()\n\nAlias of SqExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.RBFKernel","page":"Kernel Functions","title":"KernelFunctions.RBFKernel","text":"RBFKernel()\n\nAlias of SqExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.GammaExponentialKernel","page":"Kernel Functions","title":"KernelFunctions.GammaExponentialKernel","text":"GammaExponentialKernel(; γ::Real=2.0)\n\nγ-exponential kernel with parameter γ.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the γ-exponential kernel[RW] with parameter gamma in (0 2 is defined as\n\nk(x x gamma) = expbig(- x - x_2^gammabig)\n\nwarning: Warning\nThe default value of parameter γ will be changed to 1.0 in the next breaking release of KernelFunctions.\n\nSee also: ExponentialKernel, SqExponentialKernel\n\n[RW]: C. E. Rasmussen & C. K. I. Williams (2006). Gaussian Processes for Machine Learning.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Exponentiated-Kernel","page":"Kernel Functions","title":"Exponentiated Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"ExponentiatedKernel","category":"page"},{"location":"kernels/#KernelFunctions.ExponentiatedKernel","page":"Kernel Functions","title":"KernelFunctions.ExponentiatedKernel","text":"ExponentiatedKernel()\n\nExponentiated kernel.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the exponentiated kernel is defined as\n\nk(x x) = exp(x^top x)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Fractional-Brownian-Motion-Kernel","page":"Kernel Functions","title":"Fractional Brownian Motion Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"FBMKernel","category":"page"},{"location":"kernels/#KernelFunctions.FBMKernel","page":"Kernel Functions","title":"KernelFunctions.FBMKernel","text":"FBMKernel(; h::Real=0.5)\n\nFractional Brownian motion kernel with Hurst index h.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the fractional Brownian motion kernel with Hurst index h in 01 is defined as\n\nk(x x h) =  fracx_2^2h + x_2^2h - x - x^2h2\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Gabor-Kernel","page":"Kernel Functions","title":"Gabor Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"gaborkernel\nGaborKernel","category":"page"},{"location":"kernels/#KernelFunctions.gaborkernel","page":"Kernel Functions","title":"KernelFunctions.gaborkernel","text":"gaborkernel(;\n    sqexponential_transform=IdentityTransform(), cosine_tranform=IdentityTransform()\n)\n\nConstruct a Gabor kernel with transformations sqexponential_transform and cosine_transform of the inputs of the underlying squared exponential and cosine kernel, respectively.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the Gabor kernel with transformations f and g of the inputs to the squared exponential and cosine kernel, respectively, is defined as\n\nk(x x f g) = expbigg(- frac f(x) - f(x)_2^22bigg)\n                 cosbig(pi g(x) - g(x)_2 big)\n\n\n\n\n\n","category":"function"},{"location":"kernels/#KernelFunctions.GaborKernel","page":"Kernel Functions","title":"KernelFunctions.GaborKernel","text":"GaborKernel(; ell::Real=1.0, p::Real=1.0)\n\nGabor kernel with lengthscale ell and period p.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the Gabor kernel with lengthscale l_i  0 and period p_i  0 is defined as\n\nk(x x l p) = expbigg(- sum_i=1^d frac(x_i - x_i)^22l_i^2bigg)\n                 cosbigg(pi bigg(sum_i=1^d frac(x_i - x_i)^2p_i^2 bigg)^12bigg)\n\nnote: Note\nGaborKernel is deprecated and will be removed. Gabor kernels should be constructed with gaborkernel instead.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Matérn-Kernels","page":"Kernel Functions","title":"Matérn Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"MaternKernel\nMatern12Kernel\nMatern32Kernel\nMatern52Kernel","category":"page"},{"location":"kernels/#KernelFunctions.MaternKernel","page":"Kernel Functions","title":"KernelFunctions.MaternKernel","text":"MaternKernel(; ν::Real=1.5)\n\nMatérn kernel of order ν.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the Matérn kernel of order nu  0 is defined as\n\nk(xxnu) = frac2^1-nuGamma(nu)big(sqrt2nux-x_2big) K_nubig(sqrt2nux-x_2big)\n\nwhere Gamma is the Gamma function and K_nu is the modified Bessel function of the second kind of order nu.\n\nA Gaussian process with a Matérn kernel is lceil nu rceil - 1-times differentiable in the mean-square sense.\n\nSee also: Matern12Kernel, Matern32Kernel, Matern52Kernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.Matern12Kernel","page":"Kernel Functions","title":"KernelFunctions.Matern12Kernel","text":"Matern12Kernel()\n\nAlias of ExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.Matern32Kernel","page":"Kernel Functions","title":"KernelFunctions.Matern32Kernel","text":"Matern32Kernel()\n\nMatérn kernel of order 32.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the Matérn kernel of order 32 is given by\n\nk(x x) = big(1 + sqrt3 x - x_2 big) expbig(- sqrt3x - x_2big)\n\nSee also: MaternKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.Matern52Kernel","page":"Kernel Functions","title":"KernelFunctions.Matern52Kernel","text":"Matern52Kernel()\n\nMatérn kernel of order 52.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the Matérn kernel of order 52 is given by\n\nk(x x) = bigg(1 + sqrt5 x - x_2 + frac53x - x_2^2bigg)\n           expbig(- sqrt5x - x_2big)\n\nSee also: MaternKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Neural-Network-Kernel","page":"Kernel Functions","title":"Neural Network Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"NeuralNetworkKernel","category":"page"},{"location":"kernels/#KernelFunctions.NeuralNetworkKernel","page":"Kernel Functions","title":"KernelFunctions.NeuralNetworkKernel","text":"NeuralNetworkKernel()\n\nKernel of a Gaussian process obtained as the limit of a Bayesian neural network with a single hidden layer as the number of units goes to infinity.\n\nDefinition\n\nConsider the single-layer Bayesian neural network f colon mathbbR^d to mathbbR with h hidden units defined by\n\nf(x b v u) = b + sqrtfracpi2 sum_i=1^h v_i mathrmerfbig(u_i^top xbig)\n\nwhere mathrmerf is the error function, and with prior distributions\n\nbeginaligned\nb sim mathcalN(0 sigma_b^2)\nv sim mathcalN(0 sigma_v^2 mathrmI_hh)\nu_i sim mathcalN(0 mathrmI_d2) qquad (i = 1ldotsh)\nendaligned\n\nAs h to infty, the neural network converges to the Gaussian process\n\ng(cdot) sim mathcalGPbig(0 sigma_b^2 + sigma_v^2 k(cdot cdot)big)\n\nwhere the neural network kernel k is given by\n\nk(x x) = arcsinleft(fracx^top xsqrtbig(1 + x^2_2big) big(1 + x_2^2big)right)\n\nfor inputs x x in mathbbR^d.[CW]\n\n[CW]: C. K. I. Williams (1998). Computation with infinite neural networks.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Periodic-Kernel","page":"Kernel Functions","title":"Periodic Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"PeriodicKernel\nPeriodicKernel(::DataType, ::Int)","category":"page"},{"location":"kernels/#KernelFunctions.PeriodicKernel","page":"Kernel Functions","title":"KernelFunctions.PeriodicKernel","text":"PeriodicKernel(; r::AbstractVector=ones(Float64, 1))\n\nPeriodic kernel with parameter r.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the periodic kernel with parameter r_i  0 is defined[DM] as\n\nk(x x r) = expbigg(- frac12 sum_i=1^d bigg(fracsinbig(pi(x_i - x_i)big)r_ibigg)^2bigg)\n\n[DM]: D. J. C. MacKay (1998). Introduction to Gaussian Processes.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.PeriodicKernel-Tuple{DataType, Int64}","page":"Kernel Functions","title":"KernelFunctions.PeriodicKernel","text":"PeriodicKernel([T=Float64, dims::Int=1])\n\nCreate a PeriodicKernel with parameter r=ones(T, dims).\n\n\n\n\n\n","category":"method"},{"location":"kernels/#Piecewise-Polynomial-Kernel","page":"Kernel Functions","title":"Piecewise Polynomial Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"PiecewisePolynomialKernel","category":"page"},{"location":"kernels/#KernelFunctions.PiecewisePolynomialKernel","page":"Kernel Functions","title":"KernelFunctions.PiecewisePolynomialKernel","text":"PiecewisePolynomialKernel(; degree::Int=0, dim::Int)\nPiecewisePolynomialKernel{degree}(dim::Int)\n\nPiecewise polynomial kernel of degree degree for inputs of dimension dim with support in the unit ball.\n\nDefinition\n\nFor inputs x x in mathbbR^d of dimension d, the piecewise polynomial kernel of degree v in 0123 is defined as\n\nk(x x v) = max(1 - x - x 0)^alpha(vd) f_vd(x - x)\n\nwhere alpha(v d) = lfloor fracd2rfloor + 2v + 1 and f_vd are polynomials of degree v given by\n\nbeginaligned\nf_0d(r) = 1 \nf_1d(r) = 1 + (j + 1) r \nf_2d(r) = 1 + (j + 2) r + big((j^2 + 4j + 3)  3big) r^2 \nf_3d(r) = 1 + (j + 3) r + big((6 j^2 + 36j + 45)  15big) r^2 + big((j^3 + 9 j^2 + 23j + 15)  15big) r^3\nendaligned\n\nwhere j = lfloor fracd2rfloor + v + 1.\n\nThe kernel is 2v times continuously differentiable and the corresponding Gaussian process is hence v times mean-square differentiable.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Polynomial-Kernels","page":"Kernel Functions","title":"Polynomial Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"LinearKernel\nPolynomialKernel","category":"page"},{"location":"kernels/#KernelFunctions.LinearKernel","page":"Kernel Functions","title":"KernelFunctions.LinearKernel","text":"LinearKernel(; c::Real=0.0)\n\nLinear kernel with constant offset c.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the linear kernel with constant offset c geq 0 is defined as\n\nk(x x c) = x^top x + c\n\nSee also: PolynomialKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.PolynomialKernel","page":"Kernel Functions","title":"KernelFunctions.PolynomialKernel","text":"PolynomialKernel(; degree::Int=2, c::Real=0.0)\n\nPolynomial kernel of degree degree with constant offset c.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the polynomial kernel of degree nu in mathbbN with constant offset c geq 0 is defined as\n\nk(x x c nu) = (x^top x + c)^nu\n\nSee also: LinearKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Rational-Kernels","page":"Kernel Functions","title":"Rational Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"RationalKernel\nRationalQuadraticKernel\nGammaRationalKernel","category":"page"},{"location":"kernels/#KernelFunctions.RationalKernel","page":"Kernel Functions","title":"KernelFunctions.RationalKernel","text":"RationalKernel(; α::Real=2.0)\n\nRational kernel with shape parameter α.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the rational kernel with shape parameter alpha  0 is defined as\n\nk(x x alpha) = bigg(1 + fracx - x_2alphabigg)^-alpha\n\nThe ExponentialKernel is recovered in the limit as alpha to infty.\n\nSee also: GammaRationalKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.RationalQuadraticKernel","page":"Kernel Functions","title":"KernelFunctions.RationalQuadraticKernel","text":"RationalQuadraticKernel(; α::Real=2.0)\n\nRational-quadratic kernel with shape parameter α.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the rational-quadratic kernel with shape parameter alpha  0 is defined as\n\nk(x x alpha) = bigg(1 + fracx - x_2^22alphabigg)^-alpha\n\nThe SqExponentialKernel is recovered in the limit as alpha to infty.\n\nSee also: GammaRationalKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.GammaRationalKernel","page":"Kernel Functions","title":"KernelFunctions.GammaRationalKernel","text":"GammaRationalKernel(; α::Real=2.0, γ::Real=2.0)\n\nγ-rational kernel with shape parameters α and γ.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the γ-rational kernel with shape parameters alpha  0 and gamma in (0 2 is defined as\n\nk(x x alpha gamma) = bigg(1 + fracx - x_2^gammaalphabigg)^-alpha\n\nThe GammaExponentialKernel is recovered in the limit as alpha to infty.\n\nwarning: Warning\nThe default value of parameter γ will be changed to 1.0 in the next breaking release of KernelFunctions.\n\nSee also: RationalKernel, RationalQuadraticKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Spectral-Mixture-Kernels","page":"Kernel Functions","title":"Spectral Mixture Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"spectral_mixture_kernel\nspectral_mixture_product_kernel","category":"page"},{"location":"kernels/#KernelFunctions.spectral_mixture_kernel","page":"Kernel Functions","title":"KernelFunctions.spectral_mixture_kernel","text":"spectral_mixture_kernel(\n    h::Kernel=SqExponentialKernel(),\n    αs::AbstractVector{<:Real},\n    γs::AbstractMatrix{<:Real},\n    ωs::AbstractMatrix{<:Real},\n)\n\nwhere αs are the weights of dimension (A, ), γs is the covariance matrix of dimension (D, A) and ωs are the mean vectors and is of dimension (D, A). Here, D is input dimension and A is the number of spectral components.\n\nh is the kernel, which defaults to SqExponentialKernel if not specified.\n\nGeneralised Spectral Mixture kernel function. This family of functions is  dense in the family of stationary real-valued kernels with respect to the pointwise convergence.[1]\n\n   κ(x y) = αs (h(-(γs * t)^2) * cos(π * ωs * t) t = x - y\n\nReferences:\n\n[1] Generalized Spectral Kernels, by Yves-Laurent Kom Samo and Stephen J. Roberts\n[2] SM: Gaussian Process Kernels for Pattern Discovery and Extrapolation,\n        ICML, 2013, by Andrew Gordon Wilson and Ryan Prescott Adams,\n[3] Covariance kernels for fast automatic pattern discovery and extrapolation\n    with Gaussian processes, Andrew Gordon Wilson, PhD Thesis, January 2014.\n    http://www.cs.cmu.edu/~andrewgw/andrewgwthesis.pdf\n[4] http://www.cs.cmu.edu/~andrewgw/pattern/.\n\n\n\n\n\n","category":"function"},{"location":"kernels/#KernelFunctions.spectral_mixture_product_kernel","page":"Kernel Functions","title":"KernelFunctions.spectral_mixture_product_kernel","text":"spectral_mixture_product_kernel(\n    h::Kernel=SqExponentialKernel(),\n    αs::AbstractMatrix{<:Real},\n    γs::AbstractMatrix{<:Real},\n    ωs::AbstractMatrix{<:Real},\n)\n\nwhere αs are the weights of dimension (D, A), γs is the covariance matrix of dimension (D, A) and ωs are the mean vectors and is of dimension (D, A). Here, D is input dimension and A is the number of spectral components.\n\nSpectral Mixture Product Kernel. With enough components A, the SMP kernel can model any product kernel to arbitrary precision, and is flexible even with a small number of components [1]\n\nh is the kernel, which defaults to SqExponentialKernel if not specified.\n\n   κ(x y) = Πᵢ₁ᴷ Σ(αsᵢᵀ * (h(-(γsᵢᵀ * tᵢ)²) * cos(ωsᵢᵀ * tᵢ))) tᵢ = xᵢ - yᵢ\n\nReferences:\n\n[1] GPatt: Fast Multidimensional Pattern Extrapolation with GPs,\n    arXiv 1310.5288, 2013, by Andrew Gordon Wilson, Elad Gilboa,\n    Arye Nehorai and John P. Cunningham\n\n\n\n\n\n","category":"function"},{"location":"kernels/#Wiener-Kernel","page":"Kernel Functions","title":"Wiener Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"WienerKernel","category":"page"},{"location":"kernels/#KernelFunctions.WienerKernel","page":"Kernel Functions","title":"KernelFunctions.WienerKernel","text":"WienerKernel(; i::Int=0)\nWienerKernel{i}()\n\nThe i-times integrated Wiener process kernel function.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the i-times integrated Wiener process kernel with i in -1 0 1 2 3 is defined[SDH] as\n\nk_i(x x) = begincases\n    delta(x x)  textif  i=-1\n    minbig(x_2 x_2big)  textif  i=0\n    a_i1^-1 minbig(x_2 x_2big)^2i + 1\n    + a_i2^-1 x - x_2 r_ibig(x_2 x_2big) minbig(x_2 x_2big)^i + 1\n     textotherwise\nendcases\n\nwhere the coefficients a are given by\n\na = beginbmatrix\n3  2 \n20  12 \n252  720\nendbmatrix\n\nand the functions r_i are defined as\n\nbeginaligned\nr_1(t t) = 1\nr_2(t t) = t + t - fracmin(t t)2\nr_3(t t) = 5 max(t t)^2 + 2 tt + 3 min(t t)^2\nendaligned\n\nThe WhiteKernel is recovered for i = -1.\n\n[SDH]: Schober, Duvenaud & Hennig (2014). Probabilistic ODE Solvers with Runge-Kutta Means.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Composite-Kernels","page":"Kernel Functions","title":"Composite Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The modular design of KernelFunctions uses base kernels as building blocks for more complex kernels. There are a variety of composite kernels implemented, including those which transform the inputs to a wrapped kernel to implement length scales, scale the variance of a kernel, and sum or multiply collections of kernels together.","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"TransformedKernel\n∘(::Kernel, ::Transform)\nScaledKernel\nKernelSum\nKernelProduct\nKernelTensorProduct\nNormalizedKernel","category":"page"},{"location":"kernels/#KernelFunctions.TransformedKernel","page":"Kernel Functions","title":"KernelFunctions.TransformedKernel","text":"TransformedKernel(k::Kernel, t::Transform)\n\nKernel derived from k for which inputs are transformed via a Transform t.\n\nThe preferred way to create kernels with input transformations is to use the composition operator ∘ or its alias compose instead of TransformedKernel directly since this allows optimized implementations for specific kernels and transformations.\n\nSee also: ∘\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Base.:∘-Tuple{Kernel, Transform}","page":"Kernel Functions","title":"Base.:∘","text":"kernel ∘ transform\n∘(kernel, transform)\ncompose(kernel, transform)\n\nCompose a kernel with a transformation transform of its inputs.\n\nThe prefix forms support chains of multiple transformations: ∘(kernel, transform1, transform2) = kernel ∘ transform1 ∘ transform2.\n\nDefinition\n\nFor inputs x x, the transformed kernel widetildek derived from kernel k by input transformation t is defined as\n\nwidetildek(x x k t) = kbig(t(x) t(x)big)\n\nExamples\n\njulia> (SqExponentialKernel() ∘ ScaleTransform(0.5))(0, 2) == exp(-0.5)\ntrue\n\njulia> ∘(ExponentialKernel(), ScaleTransform(2), ScaleTransform(0.5))(1, 2) == exp(-1)\ntrue\n\nSee also: TransformedKernel\n\n\n\n\n\n","category":"method"},{"location":"kernels/#KernelFunctions.ScaledKernel","page":"Kernel Functions","title":"KernelFunctions.ScaledKernel","text":"ScaledKernel(k::Kernel, σ²::Real=1.0)\n\nScaled kernel derived from k by multiplication with variance σ².\n\nDefinition\n\nFor inputs x x, the scaled kernel widetildek derived from kernel k by multiplication with variance sigma^2  0 is defined as\n\nwidetildek(x x k sigma^2) = sigma^2 k(x x)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.KernelSum","page":"Kernel Functions","title":"KernelFunctions.KernelSum","text":"KernelSum <: Kernel\n\nCreate a sum of kernels. One can also use the operator +.\n\nThere are various ways in which you create a KernelSum:\n\nThe simplest way to specify a KernelSum would be to use the overloaded + operator. This is  equivalent to creating a KernelSum by specifying the kernels as the arguments to the constructor.  \n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5);\n\njulia> (k = k1 + k2) == KernelSum(k1, k2)\ntrue\n\njulia> kernelmatrix(k1 + k2, X) == kernelmatrix(k1, X) .+ kernelmatrix(k2, X)\ntrue\n\njulia> kernelmatrix(k, X) == kernelmatrix(k1 + k2, X)\ntrue\n\nYou could also specify a KernelSum by providing a Tuple or a Vector of the  kernels to be summed. We suggest you to use a Tuple when you have fewer components   and a Vector when dealing with a large number of components.\n\njulia> KernelSum((k1, k2)) == k1 + k2\ntrue\n\njulia> KernelSum([k1, k2]) == KernelSum((k1, k2)) == k1 + k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.KernelProduct","page":"Kernel Functions","title":"KernelFunctions.KernelProduct","text":"KernelProduct <: Kernel\n\nCreate a product of kernels. One can also use the overloaded operator *.\n\nThere are various ways in which you create a KernelProduct:\n\nThe simplest way to specify a KernelProduct would be to use the overloaded * operator. This is  equivalent to creating a KernelProduct by specifying the kernels as the arguments to the constructor.  \n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5);\n\njulia> (k = k1 * k2) == KernelProduct(k1, k2)\ntrue\n\njulia> kernelmatrix(k1 * k2, X) == kernelmatrix(k1, X) .* kernelmatrix(k2, X)\ntrue\n\njulia> kernelmatrix(k, X) == kernelmatrix(k1 * k2, X)\ntrue\n\nYou could also specify a KernelProduct by providing a Tuple or a Vector of the  kernels to be multiplied. We suggest you to use a Tuple when you have fewer components   and a Vector when dealing with a large number of components.\n\njulia> KernelProduct((k1, k2)) == k1 * k2\ntrue\n\njulia> KernelProduct([k1, k2]) == KernelProduct((k1, k2)) == k1 * k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.KernelTensorProduct","page":"Kernel Functions","title":"KernelFunctions.KernelTensorProduct","text":"KernelTensorProduct\n\nTensor product of kernels.\n\nDefinition\n\nFor inputs x = (x_1 ldots x_n) and x = (x_1 ldots x_n), the tensor product of kernels k_1 ldots k_n is defined as\n\nk(x x k_1 ldots k_n) = Big(bigotimes_i=1^n k_iBig)(x x) = prod_i=1^n k_i(x_i x_i)\n\nConstruction\n\nThe simplest way to specify a KernelTensorProduct is to use the overloaded tensor operator or its alias ⊗ (can be typed by \\otimes<tab>).\n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5, 2);\n\njulia> kernelmatrix(k1 ⊗ k2, RowVecs(X)) == kernelmatrix(k1, X[:, 1]) .* kernelmatrix(k2, X[:, 2])\ntrue\n\nYou can also specify a KernelTensorProduct by providing kernels as individual arguments or as an iterable data structure such as a Tuple or a Vector. Using a tuple or individual arguments guarantees that KernelTensorProduct is concretely typed but might lead to large compilation times if the number of kernels is large.\n\njulia> KernelTensorProduct(k1, k2) == k1 ⊗ k2\ntrue\n\njulia> KernelTensorProduct((k1, k2)) == k1 ⊗ k2\ntrue\n\njulia> KernelTensorProduct([k1, k2]) == k1 ⊗ k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.NormalizedKernel","page":"Kernel Functions","title":"KernelFunctions.NormalizedKernel","text":"NormalizedKernel(k::Kernel)\n\nA normalized kernel derived from k.\n\nDefinition\n\nFor inputs x x, the normalized kernel widetildek derived from kernel k is defined as\n\nwidetildek(x x k) = frack(x x)sqrtk(x x) k(x x)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Multi-output-Kernels","page":"Kernel Functions","title":"Multi-output Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"MOKernel\nIndependentMOKernel\nLatentFactorMOKernel","category":"page"},{"location":"kernels/#KernelFunctions.MOKernel","page":"Kernel Functions","title":"KernelFunctions.MOKernel","text":"MOKernel\n\nAbstract type for kernels with multiple outpus.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.IndependentMOKernel","page":"Kernel Functions","title":"KernelFunctions.IndependentMOKernel","text":"IndependentMOKernel(k::Kernel)\n\nKernel for multiple independent outputs with kernel k each.\n\nDefinition\n\nFor inputs x x and output dimensions p_x p_x, the kernel widetildek for independent outputs with kernel k each is defined as\n\nwidetildekbig((x p_x) (x p_x)big) = begincases\n    k(x x)  textif  p_x = p_x \n    0  textotherwise\nendcases\n\nMathematically, it is equivalent to a matrix-valued kernel defined as\n\nwidetildeK(x x) = mathrmdiagbig(k(x x) ldots k(x x)big) in mathbbR^m times m\n\nwhere m is the number of outputs.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.LatentFactorMOKernel","page":"Kernel Functions","title":"KernelFunctions.LatentFactorMOKernel","text":"LatentFactorMOKernel(g, e::MOKernel, A::AbstractMatrix)\n\nKernel associated with the semiparametric latent factor model.\n\nDefinition\n\nFor inputs x x and output dimensions p_x p_x, the kernel is defined as[STJ]\n\nkbig((x p_x) (x p_x)big) = sum^Q_q=1 A_p_xqg_q(x x)A_p_xq\n                                   + ebig((x p_x) (x p_x)big)\n\nwhere g_1 ldots g_Q are Q kernels, one for each latent process, e is a multi-output kernel for m outputs, and A is a matrix of weights for the kernels of size m times Q.\n\n[STJ]: M. Seeger, Y. Teh, & M. I. Jordan (2005). Semiparametric Latent Factor Models.\n\n\n\n\n\n","category":"type"},{"location":"api/#API-Library","page":"API","title":"API Library","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/","page":"API","title":"API","text":"CurrentModule = KernelFunctions","category":"page"},{"location":"api/#Functions","page":"API","title":"Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"kernelmatrix\nkernelmatrix!\nkernelmatrix_diag\nkernelmatrix_diag!\nkernelpdmat\nnystrom","category":"page"},{"location":"api/#KernelFunctions.kernelmatrix","page":"API","title":"KernelFunctions.kernelmatrix","text":"kernelmatrix(κ::Kernel, X; obsdim::Int = 2)\nkernelmatrix(κ::Kernel, X, Y; obsdim::Int = 2)\n\nCalculate the kernel matrix of X (and Y) with respect to kernel κ. obsdim = 1 means the matrix X (and Y) has size #samples x #dimension obsdim = 2 means the matrix X (and Y) has size #dimension x #samples\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kernelmatrix!","page":"API","title":"KernelFunctions.kernelmatrix!","text":"kernelmatrix!(K::AbstractMatrix, κ::Kernel, X; obsdim::Integer = 2)\nkernelmatrix!(K::AbstractMatrix, κ::Kernel, X, Y; obsdim::Integer = 2)\n\nIn-place version of kernelmatrix where pre-allocated matrix K will be overwritten with the kernel matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kernelmatrix_diag","page":"API","title":"KernelFunctions.kernelmatrix_diag","text":"kernelmatrix_diag(κ::Kernel, X; obsdim::Int = 2)\n\nCalculate the diagonal matrix of X with respect to kernel κ obsdim = 1 means the matrix X has size #samples x #dimension obsdim = 2 means the matrix X has size #dimension x #samples\n\nkernelmatrix_diag(κ::Kernel, X, Y; obsdim::Int = 2)\n\nCalculate the diagonal of kernelmatrix(κ, X, Y; obsdim) efficiently. Requires that X and Y are the same length.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kernelmatrix_diag!","page":"API","title":"KernelFunctions.kernelmatrix_diag!","text":"kernelmatrix_diag!(K::AbstractVector, κ::Kernel, X; obsdim::Int = 2)\nkernelmatrix_diag!(K::AbstractVector, κ::Kernel, X, Y; obsdim::Int = 2)\n\nIn place version of kernelmatrix_diag\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kernelpdmat","page":"API","title":"KernelFunctions.kernelpdmat","text":"kernelpdmat(k::Kernel, X::AbstractMatrix; obsdim::Int=2)\nkernelpdmat(k::Kernel, X::AbstractVector)\n\nCompute a positive-definite matrix in the form of a PDMat matrix see PDMats.jl with the cholesky decomposition precomputed. The algorithm recursively tries to add recursively a diagonal nugget until positive definiteness is achieved or until the noise is too big.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.nystrom","page":"API","title":"KernelFunctions.nystrom","text":"nystrom(k::Kernel, X::Matrix, S::Vector; obsdim::Int=defaultobs)\n\nComputes a factorization of Nystrom approximation of the square kernel matrix of data matrix X with respect to kernel k. Returns a NystromFact struct which stores a Nystrom factorization satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\nnystrom(k::Kernel, X::Matrix, r::Real; obsdim::Int=defaultobs)\n\nComputes a factorization of Nystrom approximation of the square kernel matrix of data matrix X with respect to kernel k using a sample ratio of r. Returns a NystromFact struct which stores a Nystrom factorization satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"ColVecs\nRowVecs\nMOInput\nNystromFact","category":"page"},{"location":"api/#KernelFunctions.ColVecs","page":"API","title":"KernelFunctions.ColVecs","text":"ColVecs(X::AbstractMatrix)\n\nA lightweight wrapper for an AbstractMatrix to make it behave like a vector of vectors. Each vector represents a column of the matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.RowVecs","page":"API","title":"KernelFunctions.RowVecs","text":"RowVecs(X::AbstractMatrix)\n\nA lightweight wrapper for an AbstractMatrix to make it behave like a vector of vectors. Each vector represents a row of the matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.MOInput","page":"API","title":"KernelFunctions.MOInput","text":"MOInput(x::AbstractVector, out_dim::Integer)\n\nA data type to accomodate modelling multi-dimensional output data.\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.NystromFact","page":"API","title":"KernelFunctions.NystromFact","text":"NystromFact\n\nType for storing a Nystrom factorization. The factorization contains two fields: W and C, two matrices satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\n","category":"type"},{"location":"api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]\nModule = [\"KernelFunctions\"]\nOrder = [:type, :function]","category":"page"},{"location":"theory/#Theory","page":"Theory","title":"Theory","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"See Wikipedia article","category":"page"},{"location":"create_kernel/#Custom-Kernels","page":"Custom Kernels","title":"Custom Kernels","text":"","category":"section"},{"location":"create_kernel/#Creating-your-own-kernel","page":"Custom Kernels","title":"Creating your own kernel","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions.jl contains the most popular kernels already but you might want to make your own!","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Here are a few ways depending on how complicated your kernel is:","category":"page"},{"location":"create_kernel/#SimpleKernel-for-kernel-functions-depending-on-a-metric","page":"Custom Kernels","title":"SimpleKernel for kernel functions depending on a metric","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If your kernel function is of the form k(x, y) = f(d(x, y)) where d(x, y) is a PreMetric, you can construct your custom kernel by defining kappa and metric for your kernel. Here is for example how one can define the SqExponentialKernel again :","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"struct MyKernel <: KernelFunctions.SimpleKernel end\n\nKernelFunctions.kappa(::MyKernel, d2::Real) = exp(-d2)\nKernelFunctions.metric(::MyKernel) = SqEuclidean()","category":"page"},{"location":"create_kernel/#Kernel-for-more-complex-kernels","page":"Custom Kernels","title":"Kernel for more complex kernels","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If your kernel does not satisfy such a representation, all you need to do is define (k::MyKernel)(x, y) and inherit from Kernel. For example, we recreate here the NeuralNetworkKernel:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"struct MyKernel <: KernelFunctions.Kernel end\n\n(::MyKernel)(x, y) = asin(dot(x, y) / sqrt((1 + sum(abs2, x)) * (1 + sum(abs2, y))))","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Note that the fallback implementation of the base Kernel evaluation does not use Distances.jl and can therefore be a bit slower.","category":"page"},{"location":"create_kernel/#Additional-Options","page":"Custom Kernels","title":"Additional Options","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Finally there are additional functions you can define to bring in more features:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions.iskroncompatible(k::MyKernel): if your kernel factorizes in dimensions, you can declare your kernel as iskroncompatible(k) = true to use Kronecker methods.\nKernelFunctions.dim(x::MyDataType): by default the dimension of the inputs will only be checked for vectors of type AbstractVector{<:Real}. If you want to check the dimensionality of your inputs, dispatch the dim function on your datatype. Note that 0 is the default.\ndim is called within KernelFunctions.validate_inputs(x::MyDataType, y::MyDataType), which can instead be directly overloaded if you want to run special checks for your input types.\nkernelmatrix(k::MyKernel, ...): you can redefine the diverse kernelmatrix functions to eventually optimize the computations.\nBase.print(io::IO, k::MyKernel): if you want to specialize the printing of your kernel.","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions uses Functors.jl for specifying trainable kernel parameters in a way that is compatible with the Flux ML framework. You can use Functors.@functor if all fields of your kernel struct are trainable. Note that optimization algorithms in Flux are not compatible with scalar parameters (yet), and hence vector-valued parameters should be preferred.","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"import Functors\n\nstruct MyKernel{T} <: KernelFunctions.Kernel\n    a::Vector{T}\nend\n\nFunctors.@functor MyKernel","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If only a subset of the fields are trainable, you have to specify explicitly how to (re)construct the kernel with modified parameter values by implementing Functors.functor(::Type{<:MyKernel}, x) for your kernel struct:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"import Functors\n\nstruct MyKernel{T} <: KernelFunctions.Kernel\n    n::Int\n    a::Vector{T}\nend\n\nfunction Functors.functor(::Type{<:MyKernel}, x::MyKernel)\n    function reconstruct_mykernel(xs)\n        # keep field `n` of the original kernel and set `a` to (possibly different) `xs.a`\n        return MyKernel(x.n, xs.a)\n    end\n    return (a = x.a,), reconstruct_mykernel\nend","category":"page"},{"location":"transform/#input_transforms","page":"Input Transforms","title":"Input Transforms","text":"","category":"section"},{"location":"transform/#Overview","page":"Input Transforms","title":"Overview","text":"","category":"section"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"Transforms are designed to change input data before passing it on to a kernel object.","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"It can be as standard as IdentityTransform returning the same input, or multiplying the data by a scalar with ScaleTransform or by a vector with ARDTransform. There is a more general FunctionTransform that uses a function and applies it to each input.","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"You can also create a pipeline of Transforms via ChainTransform, e.g.,","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"LowRankTransform(rand(10, 5)) ∘ ScaleTransform(2.0)","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"A transformation t can be applied to a single input x with t(x) and to multiple inputs xs with map(t, xs).","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"Kernels can be coupled with input transformations with ∘ or its alias compose. It falls back to creating a TransformedKernel but allows more optimized implementations for specific kernels and transformations.","category":"page"},{"location":"transform/#List-of-Input-Transforms","page":"Input Transforms","title":"List of Input Transforms","text":"","category":"section"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"Transform\nIdentityTransform\nScaleTransform\nARDTransform\nARDTransform(::Real, ::Integer)\nLinearTransform\nFunctionTransform\nSelectTransform\nChainTransform\nPeriodicTransform","category":"page"},{"location":"transform/#KernelFunctions.Transform","page":"Input Transforms","title":"KernelFunctions.Transform","text":"Transform\n\nAbstract type defining a transformation of the input.\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.IdentityTransform","page":"Input Transforms","title":"KernelFunctions.IdentityTransform","text":"IdentityTransform()\n\nTransformation that returns exactly the input.\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.ScaleTransform","page":"Input Transforms","title":"KernelFunctions.ScaleTransform","text":"ScaleTransform(l::Real)\n\nTransformation that multiplies the input elementwise with l.\n\nExamples\n\njulia> l = rand(); t = ScaleTransform(l); X = rand(100, 10);\n\njulia> map(t, ColVecs(X)) == ColVecs(l .* X)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.ARDTransform","page":"Input Transforms","title":"KernelFunctions.ARDTransform","text":"ARDTransform(v::AbstractVector)\n\nTransformation that multiplies the input elementwise by v.\n\nExamples\n\njulia> v = rand(10); t = ARDTransform(v); X = rand(10, 100);\n\njulia> map(t, ColVecs(X)) == ColVecs(v .* X)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.ARDTransform-Tuple{Real, Integer}","page":"Input Transforms","title":"KernelFunctions.ARDTransform","text":"ARDTransform(s::Real, dims::Integer)\n\nCreate an ARDTransform with vector fill(s, dims).\n\n\n\n\n\n","category":"method"},{"location":"transform/#KernelFunctions.LinearTransform","page":"Input Transforms","title":"KernelFunctions.LinearTransform","text":"LinearTransform(A::AbstractMatrix)\n\nLinear transformation of the input realised by the matrix A.\n\nThe second dimension of A must match the number of features of the target.\n\nExamples\n\njulia> A = rand(10, 5); t = LinearTransform(A); X = rand(5, 100);\n\njulia> map(t, ColVecs(X)) == ColVecs(A * X)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.FunctionTransform","page":"Input Transforms","title":"KernelFunctions.FunctionTransform","text":"FunctionTransform(f)\n\nTransformation that applies function f to the input.\n\nMake sure that f can act on an input. For instance, if the inputs are vectors, use f(x) = sin.(x) instead of f = sin.\n\nExamples\n\njulia> f(x) = sum(x); t = FunctionTransform(f); X = randn(100, 10);\n\njulia> map(t, ColVecs(X)) == ColVecs(sum(X; dims=1))\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.SelectTransform","page":"Input Transforms","title":"KernelFunctions.SelectTransform","text":"SelectTransform(dims)\n\nTransformation that selects the dimensions dims of the input.\n\nExamples\n\njulia> dims = [1, 3, 5, 6, 7]; t = SelectTransform(dims); X = rand(100, 10);\n\njulia> map(t, ColVecs(X)) == ColVecs(X[dims, :])\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.ChainTransform","page":"Input Transforms","title":"KernelFunctions.ChainTransform","text":"ChainTransform(ts::AbstractVector{<:Transform})\n\nTransformation that applies a chain of transformations ts to the input.\n\nThe transformation first(ts) is applied first.\n\nExamples\n\njulia> l = rand(); A = rand(3, 4); t1 = ScaleTransform(l); t2 = LinearTransform(A);\n\njulia> X = rand(4, 10);\n\njulia> map(ChainTransform([t1, t2]), ColVecs(X)) == ColVecs(A * (l .* X))\ntrue\n\njulia> map(t2 ∘ t1, ColVecs(X)) == ColVecs(A * (l .* X))\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.PeriodicTransform","page":"Input Transforms","title":"KernelFunctions.PeriodicTransform","text":"PeriodicTransform(f)\n\nTransformation that maps the input elementwise onto the unit circle with frequency f.\n\nSamples from a GP with a kernel with this transformation applied to the inputs will produce samples with frequency f.\n\nExamples\n\njulia> f = rand(); t = PeriodicTransform(f); x = rand();\n\njulia> t(x) == [sinpi(2 * f * x), cospi(2 * f * x)]\ntrue\n\n\n\n\n\n","category":"type"},{"location":"userguide/#User-guide","page":"User guide","title":"User guide","text":"","category":"section"},{"location":"userguide/#Kernel-creation","page":"User guide","title":"Kernel creation","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"To create a kernel object, choose one of the pre-implemented kernels, see Kernel Functions, or create your own, see Creating your own kernel. For example, a squared exponential kernel is created by","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"  k = SqExponentialKernel()","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"tip: How do I set the lengthscale?\nInstead of having lengthscale(s) for each kernel we use Transform objects which act on the inputs before passing them to the kernel. Note that the transforms such as ScaleTransform and ARDTransform multiply the input by a scale factor, which corresponds to the inverse of the lengthscale. For example, a lengthscale of 0.5 is equivalent to premultiplying the input by 2.0, and you can create the corresponding kernel as follows:  k = transform(SqExponentialKernel(), ScaleTransform(2.0))\n  k = transform(SqExponentialKernel(), 2.0)  # implicitly constructs a ScaleTransform(2.0)Check the Input Transforms page for more details.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"tip: How do I set the kernel variance?\nTo premultiply the kernel by a variance, you can use * with a scalar number:  k = 3.0 * SqExponentialKernel()","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"tip: How do I use a Mahalanobis kernel?\nThe MahalanobisKernel(; P=P), defined byk(x x P) = expbig(- (x - x)^top P (x - x)big)for a positive definite matrix P = Q^top Q, was removed in 0.9. Instead you can use a squared exponential kernel together with a LinearTransform of the inputs:k = transform(SqExponentialKernel(), LinearTransform(sqrt(2) .* Q))Analogously, you can combine other kernels such as the PiecewisePolynomialKernel with a LinearTransform of the inputs to obtain a kernel that is a function of the Mahalanobis distance between inputs.","category":"page"},{"location":"userguide/#Using-a-kernel-function","page":"User guide","title":"Using a kernel function","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"To evaluate the kernel function on two vectors you simply call the kernel object:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"  k = SqExponentialKernel()\n  x1 = rand(3)\n  x2 = rand(3)\n  k(x1, x2)","category":"page"},{"location":"userguide/#Creating-a-kernel-matrix","page":"User guide","title":"Creating a kernel matrix","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"Kernel matrices can be created via the kernelmatrix function or kernelmatrix_diag for only the diagonal. An important argument to give is the data layout of the input obsdim. It specifies whether the number of observed data points is along the first dimension (obsdim=1, i.e. the matrix shape is number of samples times number of features) or along the second dimension (obsdim=2, i.e. the matrix shape is number of features times number of samples), similarly to Distances.jl. If not given explicitly, obsdim defaults to 2. For example:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"  k = SqExponentialKernel()\n  A = rand(10, 5)\n  kernelmatrix(k, A, obsdim=1)  # returns a 10x10 matrix\n  kernelmatrix(k, A, obsdim=2)  # returns a 5x5 matrix\n  k(A, obsdim=1)  # Syntactic sugar","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"We also support specific kernel matrix outputs:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"For a positive-definite matrix objectPDMat from PDMats.jl, you can call the following:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"  using PDMats\n  k = SqExponentialKernel()\n  K = kernelpdmat(k, A, obsdim=1)  # PDMat","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"It will create a matrix and in case of bad conditioning will add some diagonal noise until the matrix is considered positive-definite; it will then return a PDMat object. For this method to work in your code you need to include using PDMats first.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"For a Kronecker matrix, we rely on Kronecker.jl. Here are two examples:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"using Kronecker\nx = range(0, 1, length=10)\ny = range(0, 1, length=50)\nK = kernelkronmat(k, [x, y]) # Kronecker matrix\nK = kernelkronmat(k, x, 5) # Kronecker matrix","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"Make sure that k is a kernel compatible with such constructions (with iskroncompatible(k)). Both methods will return a Kronecker matrix. For those methods to work in your code you need to include using Kronecker first.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"For a Nystrom approximation: kernelmatrix(nystrom(k, X, ρ, obsdim=1)) where ρ is the fraction of data samples used in the approximation.","category":"page"},{"location":"userguide/#Composite-kernels","page":"User guide","title":"Composite kernels","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"Sums and products of kernels are also valid kernels. They can be created via KernelSum and KernelProduct or using simple operators + and *. For example:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"  k1 = SqExponentialKernel()\n  k2 = Matern32Kernel()\n  k = 0.5 * k1 + 0.2 * k2  # KernelSum\n  k = k1 * k2  # KernelProduct","category":"page"},{"location":"userguide/#Kernel-parameters","page":"User guide","title":"Kernel parameters","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"What if you want to differentiate through the kernel parameters? This is easy even in a highly nested structure such as:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"  k = transform(\n        0.5 * SqExponentialKernel() * Matern12Kernel()\n      + 0.2 * (transform(LinearKernel(), 2.0) + PolynomialKernel()),\n      [0.1, 0.5])","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"One can access the named tuple of trainable parameters via Functors.functor from Functors.jl. This means that in practice you can implicitly optimize the kernel parameters by calling:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"using Flux\nkernelparams = Flux.params(k)\nFlux.gradient(kernelparams) do\n    # ... some loss function on the kernel ....\nend","category":"page"},{"location":"#KernelFunctions.jl","page":"Home","title":"KernelFunctions.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Model-agnostic kernel functions compatible with automatic differentiation","category":"page"},{"location":"","page":"Home","title":"Home","text":"KernelFunctions.jl is a general purpose kernel package. It aims at providing a flexible framework for creating kernels and manipulating them. The main goals of this package compared to its predecessors/concurrents in MLKernels.jl, Stheno.jl, GaussianProcesses.jl and AugmentedGaussianProcesses.jl are:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Automatic Differentation compatibility: all kernel functions should be differentiable via packages like ForwardDiff.jl or Zygote.jl\nFlexibility: operations between kernels should be fluid and easy without breaking.\nPlug-and-play: including the kernels before/after other steps should be straightforward.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The methodology of how kernels are computed is quite simple and is done in three phases :","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Transform object is applied sample-wise on every sample\nThe pairwise matrix is computed using Distances.jl by using a Metric proper to each kernel\nThe Kernel function is applied element-wise on the pairwise matrix","category":"page"},{"location":"","page":"Home","title":"Home","text":"For a quick introduction on how to use it go to User guide","category":"page"},{"location":"example/#Examples-(WIP)","page":"Examples","title":"Examples (WIP)","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Here are a few examples of known complex kernels and how to do them. Or how to use kernels in a certain context","category":"page"},{"location":"example/#Kernel-Ridge-Regression","page":"Examples","title":"Kernel Ridge Regression","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Make a simple example of kernel ridge regression","category":"page"},{"location":"example/#Gaussian-Process-Regression","page":"Examples","title":"Gaussian Process Regression","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Make a simple example of gaussian process regression","category":"page"},{"location":"example/#Deep-Kernel-Learning","page":"Examples","title":"Deep Kernel Learning","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Put a Flux neural net in front of the kernel cf. Wilson paper","category":"page"},{"location":"example/#Kernel-Selection","page":"Examples","title":"Kernel Selection","text":"","category":"section"},{"location":"example/","page":"Examples","title":"Examples","text":"Create a large collection of kernels and optimize the weights cf AISTATS 2018 paper","category":"page"}]
}
